{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1894f082",
   "metadata": {
    "id": "1894f082"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.877 Análisis de sentimentos y textos</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster Universitario en Ciencia de Datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicaciones</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PAC 1: Procesamiento y análisis de información textual\n",
    "\n",
    "En esta práctica revisaremos y aplicaremos los conocimientos aprendidos en los módulos del 1 al 2. Concretamente trataremos 3 temas.\n",
    "\n",
    "<ul>\n",
    "<li>1. Obtención de datos a partir de información textual\n",
    "<li>2. Detección de tópicos\n",
    "<li>3. Clasificación de textos\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bf012",
   "metadata": {
    "id": "bb0bf012"
   },
   "source": [
    "El propósito de la práctica es descubrir rasgos característicos de las opiniones sobre restaurantes con las herramientas explicadas en los módulos del 1 al 2. Además, veremos si es posible clasificar automáticamente una opinión como positiva o negativa con métodos de machine learning. Utilizaremos el dataset <i>restaurants_reviews.csv</i>, extraído de una plataforma de expresión de opiniones. Este dataset contiene opiniones sobre restaurantes en inglés. El dataset se organiza en 10 columnas:\n",
    "\n",
    "<b>business_id</b>: identificador del restaurant<br>\n",
    "<b>date</b>: fecha de publicación de la opinión<br>\n",
    "<b>review_id</b>: identificador de la opinión<br>\n",
    "<b>stars</b>: calificación o valoración del restaurant en estrellas<br>\n",
    "<b>text</b>: texto de la opinión<br>\n",
    "<b>type</b>: tipo de texto<br>\n",
    "<b>user_id</b>: identificador de usuario<br>\n",
    "<b>cool, useful </b> y <b>funny</b>: Número de valoraciones que han realizado los usuarios de la plataforma para estos tres criterios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1333f6b",
   "metadata": {
    "id": "d1333f6b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Desactivar els avisos\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d20de",
   "metadata": {
    "id": "3d8d20de"
   },
   "source": [
    "# Preparación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf74bac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "bbf74bac",
    "outputId": "c7cb1550-c559-48ce-d776-642870aa2716",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-yxfBYGB6SEqszmxJxd97A</td>\n",
       "      <td>2007-12-13</td>\n",
       "      <td>m2CKSsepBCoRYWxiRUsxAg</td>\n",
       "      <td>4</td>\n",
       "      <td>Quiessence is, simply put, beautiful.  Full wi...</td>\n",
       "      <td>review</td>\n",
       "      <td>sqYN3lNgvPbPCTRsMFu27g</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zp713qNhx8d9KCJJnrw1xA</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>riFQ3vxNpP4rWLk_CSri2A</td>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>review</td>\n",
       "      <td>wFweIWhv2fREZV_dYkz_1g</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars   \n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5  \\\n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  -yxfBYGB6SEqszmxJxd97A  2007-12-13  m2CKSsepBCoRYWxiRUsxAg      4   \n",
       "4  zp713qNhx8d9KCJJnrw1xA  2010-02-12  riFQ3vxNpP4rWLk_CSri2A      5   \n",
       "\n",
       "                                                text    type   \n",
       "0  My wife took me here on my birthday for breakf...  review  \\\n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Quiessence is, simply put, beautiful.  Full wi...  review   \n",
       "4  Drop what you're doing and drive here. After I...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  sqYN3lNgvPbPCTRsMFu27g     4       3      1  \n",
       "4  wFweIWhv2fREZV_dYkz_1g     7       7      4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Obrir el fitxer de comentaris:\n",
    "df = pd.read_csv('restaurants_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49915a",
   "metadata": {
    "id": "0c49915a"
   },
   "source": [
    "Para realizar la práctica, sólo necesitaremos los textos y las valoraciones en estrellas. Por tanto, eliminamos las columnas innecesarias y nos quedaremos solo con las columnas <b>stars</b> y <b>text</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656acd34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "656acd34",
    "outputId": "9d423ef9-2f99-4e01-e076-1f14eaaedf8f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quiessence is, simply put, beautiful.  Full wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text\n",
       "0      5  My wife took me here on my birthday for breakf...\n",
       "1      5  I have no idea why some people give bad review...\n",
       "2      4  love the gyro plate. Rice is so good and I als...\n",
       "3      4  Quiessence is, simply put, beautiful.  Full wi...\n",
       "4      5  Drop what you're doing and drive here. After I..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['business_id', 'review_id', 'user_id', 'date', 'type', 'funny', 'cool', 'useful'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8aceb9",
   "metadata": {
    "id": "ef8aceb9"
   },
   "source": [
    "Trabajaremos con las opiniones que tienen las calificaciones más altas y las más bajas. Las opiniones cuya calificación sea mayor que 3 serán etiquetadas con '1', mientras que etiquetaremos con '0' las opiniones que tengan una calificación menor a 3. Las etiquetas se asignan a la columna <b>sentiment</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d52608",
   "metadata": {
    "id": "16d52608",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quiessence is, simply put, beautiful.  Full wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text  sentiment\n",
       "0      5  My wife took me here on my birthday for breakf...          1\n",
       "1      5  I have no idea why some people give bad review...          1\n",
       "2      4  love the gyro plate. Rice is so good and I als...          1\n",
       "3      4  Quiessence is, simply put, beautiful.  Full wi...          1\n",
       "4      5  Drop what you're doing and drive here. After I...          1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se eliminan las calificaciones que sean igual a tres\n",
    "df = df[(df['stars'] > 3) | (df['stars'] < 3)] # Igual a df[(df['stars'] != 3)]\n",
    "\n",
    "df['sentiment'] = df['stars'].apply(lambda x : 1 if x > 3 else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6302df",
   "metadata": {
    "id": "ed6302df"
   },
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa6736",
   "metadata": {
    "id": "e7fa6736"
   },
   "source": [
    "Antes de trabajar con los textos de las opiniones, hay que limpiarlos de caracteres como los saltos de línea (e.g: *Should be called house of deliciousness!\\r\\n\\r*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2d9fd79",
   "metadata": {
    "id": "e2d9fd79",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CarriageReturnReplacer(object):\n",
    "    \"\"\" Replaces \\r\\n expressions in a text.\n",
    "    >>> replacer = CarriageReturnReplacer()\n",
    "    >>> replacer.replace(\"\\r\\n\\r\\nAnyway, I can\\'t wait to go back!\")\n",
    "    'Anyway, I can\\'t wait to go back!'\n",
    "    \"\"\"\n",
    "    \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        # También se puede usar re.sub(r\"(\\r|\\n)\", \"\", s)\n",
    "        s = s.replace('\\r\\n', ' ')\n",
    "        s = s.replace('\\n\\n', ' ') \n",
    "        s = s.replace('\\n', ' ')\n",
    "        s = s.replace('\\r', ' ') \n",
    "        return s\n",
    "\n",
    "newline_replacer = CarriageReturnReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b890c968",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "b890c968",
    "outputId": "9e7cfcbf-eef2-4f99-aa60-eb6a9c40a10d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quiessence is, simply put, beautiful.  Full wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text  sentiment\n",
       "0      5  My wife took me here on my birthday for breakf...          1\n",
       "1      5  I have no idea why some people give bad review...          1\n",
       "2      4  love the gyro plate. Rice is so good and I als...          1\n",
       "3      4  Quiessence is, simply put, beautiful.  Full wi...          1\n",
       "4      5  Drop what you're doing and drive here. After I...          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(newline_replacer.replace)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779063f",
   "metadata": {
    "id": "6779063f"
   },
   "source": [
    "También es necesario eliminar los doble espacios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "178a5e7d",
   "metadata": {
    "id": "178a5e7d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class ExtraSpacesReplacer(object):\n",
    "    \"\"\" Replaces extra spaces in a text.\n",
    "    >>> replacer = ExtraSpacesReplacer()\n",
    "    >>> replacer.replace(\"and it was excellent.  The weather was perfect\")\n",
    "    'and it was excellent. The weather was perfect'\n",
    "    \"\"\"\n",
    "    \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        s = re.sub('\\s\\s+', ' ', s)\n",
    "        return s\n",
    "\n",
    "spaces_replacer = ExtraSpacesReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e85adcdc",
   "metadata": {
    "id": "e85adcdc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quiessence is, simply put, beautiful. Full win...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text  sentiment\n",
       "0      5  My wife took me here on my birthday for breakf...          1\n",
       "1      5  I have no idea why some people give bad review...          1\n",
       "2      4  love the gyro plate. Rice is so good and I als...          1\n",
       "3      4  Quiessence is, simply put, beautiful. Full win...          1\n",
       "4      5  Drop what you're doing and drive here. After I...          1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(spaces_replacer.replace)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd14cd0",
   "metadata": {
    "id": "9dd14cd0"
   },
   "source": [
    "# 1. Obtención de datos a partir de información textual (4 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b68be",
   "metadata": {
    "id": "312b68be"
   },
   "source": [
    "## 1.1 Encontrar colocaciones (2 puntos)\n",
    "\n",
    "Recordemos que las colocaciones son términos multipalabra, es decir, secuencias de palabras que, en conjunto, tienen un significado que difiere significativamente del significado de cada palabra individual (e.g. New York tiene un significado distinto del que se puede derivar de New y de York)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41ffd0",
   "metadata": {
    "id": "de41ffd0"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong>  Calcula los mejores bigramas y trigramas de las opiniones. (1 punto)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "lk582wJzf5UM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lk582wJzf5UM",
    "outputId": "b419a309-88b8-496a-8944-e5db4806d0be",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Para este apartado es necesario cargar las siguientes librerías:\n",
    "import nltk\n",
    "#nltk.download('all')\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.collocations import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6bfa1a6",
   "metadata": {
    "id": "f6bfa1a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importar la lista de stopwords en inglés de la libreria NLTK.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# Añadir stopwords\n",
    "stopwords = stopwords + ['unknown', 've', 'hadn', 'll', 'didn', 'isn', 'doesn', 'hasn' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d8030",
   "metadata": {
    "id": "f79d8030"
   },
   "source": [
    "A partir del comando help(nltk.collocations.BigramAssocMeasures) explora la clase BigramAssocMeasures del módulo nltk.metrics.association y revisa las definiciones de las métricas de Likelihood Ratio (likelihood_ratio) y de Pointwise Mutual Information (pmi) se explica en el capítulo 5 del libro Foundations of Statistical Natural Language Processing (Manning & Schutze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35defbdd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35defbdd",
    "outputId": "e29865b3-cdb8-42cc-8b61-0a8c1c3ae002",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BigramAssocMeasures in module nltk.metrics.association:\n",
      "\n",
      "class BigramAssocMeasures(NgramAssocMeasures)\n",
      " |  A collection of bigram association measures. Each association measure\n",
      " |  is provided as a function with three arguments::\n",
      " |  \n",
      " |      bigram_score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
      " |  \n",
      " |  The arguments constitute the marginals of a contingency table, counting\n",
      " |  the occurrences of particular events in a corpus. The letter i in the\n",
      " |  suffix refers to the appearance of the word in question, while x indicates\n",
      " |  the appearance of any word. Thus, for example:\n",
      " |  \n",
      " |  - n_ii counts ``(w1, w2)``, i.e. the bigram being scored\n",
      " |  - n_ix counts ``(w1, *)``\n",
      " |  - n_xi counts ``(*, w2)``\n",
      " |  - n_xx counts ``(*, *)``, i.e. any bigram\n",
      " |  \n",
      " |  This may be shown with respect to a contingency table::\n",
      " |  \n",
      " |              w1    ~w1\n",
      " |           ------ ------\n",
      " |       w2 | n_ii | n_oi | = n_xi\n",
      " |           ------ ------\n",
      " |      ~w2 | n_io | n_oo |\n",
      " |           ------ ------\n",
      " |           = n_ix        TOTAL = n_xx\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BigramAssocMeasures\n",
      " |      NgramAssocMeasures\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  chi_sq(n_ii, n_ix_xi_tuple, n_xx) from abc.ABCMeta\n",
      " |      Scores bigrams using chi-square, i.e. phi-sq multiplied by the number\n",
      " |      of bigrams, as in Manning and Schutze 5.3.3.\n",
      " |  \n",
      " |  fisher(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using Fisher's Exact Test (Pedersen 1996).  Less\n",
      " |      sensitive to small counts than PMI or Chi Sq, but also more expensive\n",
      " |      to compute. Requires scipy.\n",
      " |  \n",
      " |  phi_sq(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using phi-square, the square of the Pearson correlation\n",
      " |      coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  dice(n_ii, n_ix_xi_tuple, n_xx)\n",
      " |      Scores bigrams using Dice's coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  jaccard(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Jaccard index.\n",
      " |  \n",
      " |  likelihood_ratio(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using likelihood ratios as in Manning and Schutze 5.3.4.\n",
      " |  \n",
      " |  pmi(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams by pointwise mutual information, as in Manning and\n",
      " |      Schutze 5.4.\n",
      " |  \n",
      " |  poisson_stirling(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Poisson-Stirling measure.\n",
      " |  \n",
      " |  student_t(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using Student's t test with independence hypothesis\n",
      " |      for unigrams, as in Manning and Schutze 5.3.1.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  mi_like(*marginals, **kwargs)\n",
      " |      Scores ngrams using a variant of mutual information. The keyword\n",
      " |      argument power sets an exponent (default 3) for the numerator. No\n",
      " |      logarithm of the result is calculated.\n",
      " |  \n",
      " |  raw_freq(*marginals)\n",
      " |      Scores ngrams by their frequency\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.collocations.BigramAssocMeasures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f7371",
   "metadata": {
    "id": "837f7371"
   },
   "source": [
    "<i>Primer paso</i>: Obtener los tokens del texto de las opiniones. Etiqueta estos tokens por su PoS. (0.5 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314750c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "314750c1",
    "outputId": "5fcd023c-cf78-4308-e27c-931a935baa9e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"my wife took me here on my birthday for breakfast and it was excellent. the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure. our waitress was excellent and our food arrived quickly on the semi-busy saturday morning. it looked like the place fills up pretty quickly so the earlier you get here the better. do yourself a favor and get their bloody mary. it was phenomenal and simply the best i've ever had. i'm pretty sure they only use ingredients from th\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos texto en minúscula que recoja todas las opiniones\n",
    "opinions = \" \".join(df['text']).lower()\n",
    "opinions[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c27f9b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c27f9b8",
    "outputId": "1d989339-4a06-4a50-9629-dfd48ad803a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                   #\n",
    "#############################################\n",
    "# Dividir las opiniones en tokens\n",
    "opinions_tokens = [word for word in word_tokenize(opinions)]\n",
    "\n",
    "# PoS tagging de los tokens\n",
    "opinions_tagged_tokens = nltk.pos_tag(opinions_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c52baf5f-f905-4bc3-9df5-d8c4326e87a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('my', 'PRP$'),\n",
       " ('wife', 'NN'),\n",
       " ('took', 'VBD'),\n",
       " ('me', 'PRP'),\n",
       " ('here', 'RB'),\n",
       " ('on', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('birthday', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('breakfast', 'NN')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions_tagged_tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7287ecb",
   "metadata": {
    "id": "c7287ecb"
   },
   "source": [
    "<i>Segundo paso</i>: Calcular los 1000 mejores bigramas y los 1000 mejores trigramas a partir de los tokens etiquetados (e.g. [(Basic, JJ), ...]) del texto. Utiliza la métrica PMI o Likehood Ratio. Tienes que indicar por qué has escogido una y no otra. (0.5 puntos)\n",
    "\n",
    "<b>Atención</b>: De los 1000 bigramas y trigramas, elige a los que no comienzan ni terminan con una stopword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2fe029",
   "metadata": {
    "id": "de2fe029"
   },
   "source": [
    "Recordemos la clasificación de etiquetas PoS.\n",
    "\n",
    "<b>Etiquetas PoS</b>\n",
    "\n",
    "<ul>\n",
    "<li>DT: Determinante</li>\n",
    "<li>JJ: Adjetivo</li>\n",
    "<li>NN: Nombre en singular</li>\n",
    "<li>NNS: Nombre en plural</li>\n",
    "<li>VBD: Verbo en pasado</li>\n",
    "<li>VBG: Verbo en gerundio</li>\n",
    "<li>MD: Verbo modal</li>\n",
    "<li>IN: Preposición o conjunción subordinada</li>\n",
    "<li>PRP: Pronombre</li>\n",
    "<li>RB: Adverbio</li>\n",
    "<li>RP: Partícula</li>    \n",
    "<li>CC: Conjunción coordinada</li>\n",
    "<li>CD: Numeral</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9dfdc-ace0-40ed-b3fc-33e5c86d32a9",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/179010/difference-between-pointwise-mutual-information-and-log-likelihood-ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43dfad33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43dfad33",
    "outputId": "0521a232-af8c-4479-c1c8-bc471d69eaa5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "# Cargamos las métricas para el cálculo de bigramas y trigramas\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Cargamos ngrams para enseñar ejeplos de ngrmas\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83e7594a-6889-4a93-a58a-188a44551901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGRAMAS\n",
      "[('my', 'wife'), ('wife', 'took'), ('took', 'me'), ('me', 'here'), ('here', 'on'), ('on', 'my'), ('my', 'birthday'), ('birthday', 'for'), ('for', 'breakfast'), ('breakfast', 'and'), ('and', 'it'), ('it', 'was'), ('was', 'excellent'), ('excellent', '.'), ('.', 'the')]\n",
      "\n",
      "TRIGRAMAS\n",
      "[('my', 'wife', 'took'), ('wife', 'took', 'me'), ('took', 'me', 'here'), ('me', 'here', 'on'), ('here', 'on', 'my'), ('on', 'my', 'birthday'), ('my', 'birthday', 'for'), ('birthday', 'for', 'breakfast'), ('for', 'breakfast', 'and'), ('breakfast', 'and', 'it'), ('and', 'it', 'was'), ('it', 'was', 'excellent'), ('was', 'excellent', '.'), ('excellent', '.', 'the'), ('.', 'the', 'weather')]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de bigramas\n",
    "opinion_bigrams = ngrams(opinions_tokens, 2)\n",
    "print(\"BIGRAMAS\")\n",
    "print(str(list(opinion_bigrams)[0:15]))\n",
    "\n",
    "# Ejemplo de trigramas\n",
    "opinion_trigrams = ngrams(opinions_tokens, 3)\n",
    "print()\n",
    "print(\"TRIGRAMAS\")\n",
    "print(str(list(opinion_trigrams)[0:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa204254-4cd3-4233-a572-89fa400184ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 mejores bigramas\n",
      "[(\"'eggless\", 'eggrolss'), (\"'how\", \"'bout\"), (\"'real\", \"food'/a\"), (\"'visual\", 'fluffers'), ('-50', 'gauges'), ('-elizabeth', 'rozin'), ('-or-', 'tinga'), ('.............', 'yeppie'), ('/glenlivit', '/glenmorangie'), ('1/16', 'teaspoon'), ('1202', 'mohave'), ('1980s', 'pseudo-asian'), ('1min', '45sec'), ('2-4-1.', 'woot'), ('4/', 'beach-head'), ('49er', 'flapjacks'), ('4th-tier', 'mid-90'), ('50¢', 'addtl'), ('65+', 'eyefucking'), ('=shiner', 'bock'), ('aa', 'meeting-'), ('abe', 'frohman'), ('administrative', 'assistant'), ('advent', 'epicurean'), ('afterglow', 'punctuated'), ('alain', 'keller'), ('alessandro', 'marchesan'), ('altering', 'drugs'), ('ambience=four', 'estrellas')]\n",
      "\n",
      "1000 mejores trigramas\n",
      "[('4/', 'beach-head', '2000'), ('bahay', 'kubo', 'natin'), ('bar/boxing', 'ring/vintage', 'store/food'), ('canh', 'chua', 'tom.i'), ('cheese/jewish', 'sliders/patty', 'melt/kosher'), ('dead-eyed', 'gamblers', 'shuffling'), ('donec', 'obviam', 'redimus'), ('egg-foooo', 'youngggg', 'burrritos'), ('furry', 'scavengers', 'vie'), ('glasgow', 'celtic', 'supporters-'), ('glenfiddich', '/glenlivit', '/glenmorangie'), ('hui', 'guo', 'rou'), ('iridescent', 'big-mouth', 'carp'), ('kraut/knish/fresh', 'chips/potato', 'salad/macaroni'), ('luana', 'joya', 'lucia'), ('mechwarrior', '4/', 'beach-head'), ('munchener', 'spaten', 'pils'), ('mì', 'bò', 'kho'), ('nancy', 'bartholomew', 'novles'), ('over-fried', 'french-fry-shaped', 'fry-shells'), ('pay-', '50¢', 'addtl'), ('powered/laser', 'sighted', 'rifle'), ('ring/vintage', 'store/food', 'truck/soda'), ('shank-', 'angello', 'dorato'), ('spewed', 'racial', 'slurs'), ('spinto', 'band/art', 'brut/we'), ('bi', 'bim', 'bab'), ('bilbo', 'baggins-ey', 'cauldron'), ('flee', 'pell', 'mell')]\n"
     ]
    }
   ],
   "source": [
    "# Cálculo del 1000 mejores bigramas\n",
    "bigram_candidates = BigramCollocationFinder.from_words(opinions_tokens)\n",
    "best_1000_bigram_candidates = bigram_candidates.nbest(bigram_measures.pmi, 1000)\n",
    "print(\"1000 mejores bigramas\")\n",
    "print(list(best_1000_bigram_candidates)[1:30])\n",
    "print()\n",
    "\n",
    "# Cálculo del 1000 mejores trigramas\n",
    "trigram_candidates = TrigramCollocationFinder.from_words(opinions_tokens)\n",
    "best_1000_trigram_candidates = trigram_candidates.nbest(trigram_measures.pmi, 1000)\n",
    "print(\"1000 mejores trigramas\")\n",
    "print(list(best_1000_trigram_candidates)[1:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d218f-cdca-4825-9086-e3a4b713373a",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">Para filtrar las stopwords de los candidados, emplearemos las funciones definidas en la <b>PLA1</b> de la asignatura (Módulo 1-Cómo interpretar y analizar automáticamente la información textual, Joaquim More).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ba36da9-7843-40fb-8c3b-df26aa09d785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Función para filtrar los tokens que sean stopwords\n",
    "def stopword_candidate(candidate):\n",
    "    test = True\n",
    "    if candidate[0] in stopwords or candidate[-1] in stopwords:\n",
    "        test = False\n",
    "    return test\n",
    "\n",
    "# Función para filtrar los tokens\n",
    "def filter_candidates(candidates, stopwords):\n",
    "    #Si hemos cargado una lista de stopwords\n",
    "    if len(stopwords) > 0:\n",
    "         #Creamos una lista de candidatos que pasan el test de stopwords\n",
    "        filtered_candidates = [candidate for candidate in candidates if stopword_candidate(candidate) == True]\n",
    "    else:\n",
    "        filtered_candidates = candidates\n",
    "    return filtered_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "423e9a4d-9688-4933-8a4d-b779f96e70e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 mejores bigramas filtrados\n",
      "[(\"'eggless\", 'eggrolss'), (\"'how\", \"'bout\"), (\"'real\", \"food'/a\"), (\"'visual\", 'fluffers'), ('-50', 'gauges'), ('-elizabeth', 'rozin'), ('-or-', 'tinga'), ('.............', 'yeppie'), ('/glenlivit', '/glenmorangie'), ('1/16', 'teaspoon'), ('1202', 'mohave'), ('1980s', 'pseudo-asian'), ('1min', '45sec'), ('2-4-1.', 'woot'), ('4/', 'beach-head'), ('49er', 'flapjacks'), ('4th-tier', 'mid-90'), ('50¢', 'addtl'), ('65+', 'eyefucking'), ('=shiner', 'bock'), ('aa', 'meeting-'), ('abe', 'frohman'), ('administrative', 'assistant'), ('advent', 'epicurean'), ('afterglow', 'punctuated'), ('alain', 'keller'), ('alessandro', 'marchesan'), ('altering', 'drugs'), ('ambience=four', 'estrellas')]\n",
      "\n",
      "1000 mejores trigramas filtrados\n",
      "[('4/', 'beach-head', '2000'), ('bahay', 'kubo', 'natin'), ('bar/boxing', 'ring/vintage', 'store/food'), ('canh', 'chua', 'tom.i'), ('cheese/jewish', 'sliders/patty', 'melt/kosher'), ('dead-eyed', 'gamblers', 'shuffling'), ('donec', 'obviam', 'redimus'), ('egg-foooo', 'youngggg', 'burrritos'), ('furry', 'scavengers', 'vie'), ('glasgow', 'celtic', 'supporters-'), ('glenfiddich', '/glenlivit', '/glenmorangie'), ('hui', 'guo', 'rou'), ('iridescent', 'big-mouth', 'carp'), ('kraut/knish/fresh', 'chips/potato', 'salad/macaroni'), ('luana', 'joya', 'lucia'), ('mechwarrior', '4/', 'beach-head'), ('munchener', 'spaten', 'pils'), ('mì', 'bò', 'kho'), ('nancy', 'bartholomew', 'novles'), ('over-fried', 'french-fry-shaped', 'fry-shells'), ('pay-', '50¢', 'addtl'), ('powered/laser', 'sighted', 'rifle'), ('ring/vintage', 'store/food', 'truck/soda'), ('shank-', 'angello', 'dorato'), ('spewed', 'racial', 'slurs'), ('spinto', 'band/art', 'brut/we'), ('bi', 'bim', 'bab'), ('bilbo', 'baggins-ey', 'cauldron'), ('flee', 'pell', 'mell')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar bigramas que comiencen por una stopword\n",
    "best_1000_bigram_candidates_filtered = filter_candidates(best_1000_bigram_candidates, stopwords)\n",
    "print(\"1000 mejores bigramas filtrados\")\n",
    "print(list(best_1000_bigram_candidates_filtered)[1:30])\n",
    "print()\n",
    "\n",
    "# Filtrar trigramas que comiencen por una stopword\n",
    "best_1000_trigram_candidates_filtered = filter_candidates(best_1000_trigram_candidates, stopwords)\n",
    "print(\"1000 mejores trigramas filtrados\")\n",
    "print(list(best_1000_trigram_candidates_filtered)[1:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a045e5d-67d3-46b2-9808-89d6de82af3b",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">Como podemos ver, hay signos de puntuación al principio o final de los tokens, por eso filtraría también estos ngramas. Para ello, se podrían eliminar estos símbolos de las opiniones, como se realiza a continuación.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ecf2f7e-ec6d-4d72-88ad-f7ec3b6419a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#stopwords = stopwords + ['\"',\"'\", '.', ',', ';', ':','-', '(', ')', '!', '?', '-', '‘', '’', '[', ']']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7c0f84f-e7c2-4c6d-9424-6739a5b55a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 mejores bigramas filtrados\n",
      "[('08', 'cakebread'), ('1/16', 'teaspoon'), ('1202', 'mohave'), ('1980s', 'pseudo-asian'), ('1min', '45sec'), ('2-4-1', 'woot'), ('4/', 'beach-head'), ('49er', 'flapjacks'), ('4th-tier', 'mid-90'), ('50¢', 'addtl'), ('65+', 'eyefucking'), ('abe', 'frohman'), ('administrative', 'assistant'), ('advent', 'epicurean'), ('afterglow', 'punctuated'), ('alain', 'keller'), ('alessandro', 'marchesan'), ('altering', 'drugs'), ('ambience=four', 'estrellas'), ('angello', 'dorato'), ('anita', 'bryant'), ('anxiously', 'awaiting'), ('authoritarian', 'overlords'), ('auto', 'auction'), ('avid', 'youth'), ('azucena', 'tovar'), ('bacon/bleu', 'cheese/walnut'), ('bahay', 'kubo'), ('band/art', 'brut/we')]\n"
     ]
    }
   ],
   "source": [
    "# Eliminar caracteres especiales de los bigramas\n",
    "opinions_tokens_cleaned = [token.strip('\".,;:-():!?-‘’  =\\'') for token in opinions_tokens]\n",
    "                                 \n",
    "bigram_candidates = BigramCollocationFinder.from_words(opinions_tokens_cleaned)\n",
    "best_1000_bigram_candidates = bigram_candidates.nbest(bigram_measures.pmi, 1000)\n",
    "\n",
    "best_1000_bigram_candidates_filtered = filter_candidates(best_1000_bigram_candidates, stopwords)\n",
    "print(\"1000 mejores bigramas filtrados\")\n",
    "print(list(best_1000_bigram_candidates_filtered)[1:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3883d1ca",
   "metadata": {
    "id": "3883d1ca"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong>  Detectar ngramas que cumplen el patrón sintáctico de un sintagma nominal (e.g: adjetivo + nombre en singular/plural y nombre + nombre) (0.5 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa8d9a-3efa-40cd-a2aa-75b3e3bc627d",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">En este caso también se emplearan algunas de las funiones definidas en la <b>PLA1</b> de la asignatura, para detectar ngramas que cumplen un patrón sintáctico dado.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64b9ebdb-f6a6-4bfc-b454-92b3386649b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patterns_and_parser():\n",
    "    #Definición de los patterns sintácticos de un nodo N.\n",
    "    jj_nns_patterns = \"\"\"\n",
    "              N: {<JJ> <NN> | <JJ> <NNS>}\n",
    "              \"\"\"\n",
    "    #Definición de los patterns sintácticos de un nodo CN. \n",
    "    cn_patterns = \"\"\"\n",
    "              CN: {<NN> <NN>}\n",
    "              \"\"\"\n",
    "    #Definición del parser que creará los nodos N del árbol\n",
    "    jj_nns_parser = nltk.RegexpParser(jj_nns_patterns)\n",
    "    \n",
    "    #Definición del parser que creará los nodos CN del árbol    \n",
    "    cn_parser = nltk.RegexpParser(cn_patterns)\n",
    "    return jj_nns_patterns, cn_patterns, jj_nns_parser, cn_parser\n",
    "\n",
    "\n",
    "def get_string_form(tuple_list):\n",
    "    words = [cti[0] for cti in tuple_list]\n",
    "    string_form = \"_\".join(words)\n",
    "    return string_form\n",
    "\n",
    "\n",
    "def get_n_and_cn(tagged_tokens):\n",
    "    #Definición de patterns sintácticos y parser\n",
    "    n_patterns, cn_patterns, n_parser, cn_parser = patterns_and_parser()\n",
    "    \n",
    "    #Creación de los árboles con los nodos N\n",
    "    n_tree = n_parser.parse(tagged_tokens)\n",
    "    \n",
    "    #Creación de los árboles con los nodos CN\n",
    "    cn_tree = cn_parser.parse(tagged_tokens)\n",
    "    \n",
    "    #Listado de las hojas de los nodos N\n",
    "    n_leaves = [s.leaves() for s in n_tree.subtrees() if s.label() == 'N']\n",
    "    \n",
    "    #Listado de las hojas de los nodos CN\n",
    "    cn_leaves = [s.leaves() for s in cn_tree.subtrees() if s.label() == 'CN'] \n",
    "\n",
    "    #Unión de los dos listados\n",
    "    n_cn_tuples = n_leaves + cn_leaves   \n",
    "    \n",
    "    #Conversión de las tuplas que representan las hojas al término\n",
    "    n_and_cn = [get_string_form(c) for c in n_cn_tuples]\n",
    "    \n",
    "    return n_and_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50c96d2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50c96d2e",
    "outputId": "eb3364ca-3cbd-4e5c-cee0-13740827e5c6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "n_and_cn = get_n_and_cn(opinions_tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b3db8ff-5bd5-4984-912b-31a833642861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolute_pleasure', 'saturday_morning', 'pretty_sure', 'white_truffle', 'vegetable_skillet', 'bad_reviews', 'own_fault', 'many_people', 'past_sunday', 'sunday_evening', 'baked_spaghetti', 'sweetish_sauce', 'bad_reviewers', 'bad_reviewers', 'serious_issues', 'full_windows', 'earthy_wooden', 'tuesday_evening', 'couple_days', 'fresh_veggies', 'much_i', 'raw_radishes', 'long_time', 'warm_foccacia', 'pomegranate_slices', 'vegetarian_entrees', 'pear_cake', 'next_day', 'green_building', 'grand_opening', 'yelping_soul', 'new_place', 'la_condesa', 'dogfish_shark', 'delicious_meals', 'much_flavor', 'hubbys_mole', 'mahi_mahi', 'burros-_mmmm', 'salsa_bar', 'strawberry_salsa', 'big_wimp', 'hot_peppers', 'yummy_bonus', 'good_food', 'mexican_folk', 'salsa_bar', 'happy_hour', 'great_atmosphere', 'wait_staff', 'apollo_beach', 'unique_talents', 'vietnamese_sandwich', 'sandwich_choices', 'modest_selection', 'baked_goods', 'atm_card', 'limited_time', 'other_things', 'irish_bars', 'wednesday_night', 'friendly_folks', 'i_ate', 'whole_bunch', 'small_rosie', 'other_toppings', 'whole_lot', 'cold_smithwick', 'whole_tab', 'fabulous_mix', 'live_band', 'female_vocalists', 'sure_i', 'small_pizza', 'bad_service', 'small_pizza', 'large_pizza', 'large_salad', 'first_time', 'little_chicken', 'hot_pepper', 'healthy_eating', 'skeptical_friends', 'shrimp_dumplings', 'many_times', 'next_table', 'whole_plant', 'favorite_place', 'true_food', 'in-depth_reviewers', 'yellow_curry', 'authentic_name', 'first_time', 'good_lunch', 'good_place', 'entire_place', 'tuesday_morning', 'hot_sauce', 'great_spicy', 'home-made_tasting']\n"
     ]
    }
   ],
   "source": [
    "print(n_and_cn[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c0a10",
   "metadata": {
    "id": "535c0a10"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Detectar colocaciones con un modelo de detección de frases, con el módulo Phraser de Gensim. Entrena el modelo con todas las opiniones (0,5 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7edb87",
   "metadata": {
    "id": "3a7edb87"
   },
   "source": [
    "<i>Primer paso</i>: Convertir las opiniones en una lista de phrases. Las phrases no deben ser stopwords. Tampoco deben empezar ni acabar con una stopword. (0.5 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3eba6bcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eba6bcc",
    "outputId": "9c939aa8-9501-4368-905e-1818776a4c7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My wife took me here on my birthday for breakfast and it was excellent',\n",
       " 'The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure',\n",
       " 'Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning',\n",
       " 'It looked like the place fills up pretty quickly so the earlier you get here the better',\n",
       " 'Do yourself a favor and get their Bloody Mary',\n",
       " \"It was phenomenal and simply the best I've ever had\",\n",
       " \"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it\",\n",
       " 'It was amazing',\n",
       " 'While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious',\n",
       " 'It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos las frases del texto\n",
    "opinions_string = \" \".join(df['text'])\n",
    "opinion_sentences = opinions_string.split('. ')\n",
    "opinion_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6ac8863-e49f-4385-afdc-a730694a1f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                 #\n",
    "#############################################\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Convertimos las frases en un stream sin stopwords\n",
    "opinion_sentences_stream = [word_tokenize(opinion.lower()) for opinion in opinion_sentences]\n",
    "opinion_sentences_stream_no_stopwords = [candidate for candidate in opinion_sentences_stream if (len(candidate) > 0 and stopword_candidate(candidate) != True and not str(candidate) in stopwords) ]\n",
    "\n",
    "# Convertimos el stream en Phrases\n",
    "opinion_phrases = Phrases(opinion_sentences_stream_no_stopwords, min_count=1, threshold=2, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10773379-2d11-4b22-8901-776d6282d52c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my wife', 'took me', 'here', 'on', 'my birthday', 'for breakfast', 'and', 'it was', 'excellent', '.', 'the weather', 'was perfect', 'which made', 'sitting outside', 'overlooking', 'their', 'grounds', 'an absolute', 'pleasure', '.']\n"
     ]
    }
   ],
   "source": [
    "document_tokens = word_tokenize(opinions_string.lower())\n",
    "opinion_phrases_no_stopwords = opinion_phrases[document_tokens]\n",
    "\n",
    "#Imprimimos las primeras 5 Phrases\n",
    "print(opinion_phrases_no_stopwords[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e13a71",
   "metadata": {
    "id": "d4e13a71"
   },
   "source": [
    "## 1.2 Vectorizar palabras y términos (2 puntos)\n",
    "\n",
    "Exploraremos la vectorización de palabras y términos con el método Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391acd67",
   "metadata": {
    "id": "391acd67"
   },
   "source": [
    "Recordemos que el paquete gensim implementa un método para entrenar modelos Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4OGwpBv8fOtp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OGwpBv8fOtp",
    "outputId": "28c2a902-4a7d-46c8-8309-4021ed74338d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which made',\n",
       " 'pleasure',\n",
       " 'like',\n",
       " 'place',\n",
       " 'fills up',\n",
       " 'earlier',\n",
       " 'better',\n",
       " 'phenomenal',\n",
       " 'pretty sure',\n",
       " 'fresh']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quitar espacios del texto\n",
    "opinion_phrases_stripped_no_stopwords = [c.strip() for c in opinion_phrases_no_stopwords]\n",
    "opinion_phrases_stripped_no_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aaa498",
   "metadata": {
    "id": "20aaa498"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "Ejercicio:</strong> Obtener targets de las opiniones y sus aspectos utilizando el modelo word2vec (2 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d68931",
   "metadata": {
    "id": "a9d68931"
   },
   "source": [
    "<i>Primer paso</i>: Convertir las phrases de cada oración en un token. Lo haremos concatenando los tokens de la phrase con el caracter '_' (e.g: 'scrambled eggs' -> 'scrambled_eggs'). Entonces, en cada oración sustituimos los bigramas que son phrases por la forma tokenizada (e.g: I made scrambled eggs -> I made scrambled_eggs). De esta forma, las colocaciones formarán parte del vocabulario del modelo word2vec que generaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dee08237",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dee08237",
    "outputId": "a0b73667-e6f2-4bb9-8da6-f6c62ac375e6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My wife took me here on my birthday for breakfast and it was excellent',\n",
       " 'The weather was perfect which_made sitting outside overlooking their grounds an absolute pleasure',\n",
       " 'Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning',\n",
       " 'It looked_like the place fills_up pretty quickly so the earlier you get here the better',\n",
       " 'Do yourself a favor and get their Bloody Mary',\n",
       " \"It was_phenomenal and simply the best I've ever had\",\n",
       " \"I'm pretty_sure they only use ingredients from their garden and blend them fresh when_you order it\",\n",
       " 'It was_amazing',\n",
       " 'While EVERYTHING on the menu looks excellent, I had the white_truffle scrambled eggs vegetable skillet and it was tasty and delicious',\n",
       " 'It came_with 2 pieces_of their griddled bread with was_amazing and it absolutely made the meal complete']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "import gensim\n",
    "\n",
    "# Filter collocations\n",
    "collocation_phrases = [phrase for phrase in list(set(opinion_phrases_stripped_no_stopwords)) if ' ' in phrase]\n",
    "\n",
    "def transform_sentence(sentence):\n",
    "    transformed_sentence = sentence\n",
    "    n_grams = list(ngrams(nltk.word_tokenize(sentence), 2))\n",
    "    ngrams_t = [' '.join(gram) for gram in n_grams]\n",
    "    for ngram in ngrams_t:\n",
    "        if ngram in collocation_phrases:\n",
    "            opt = ngram.replace(' ', '_')\n",
    "            transformed_sentence = transformed_sentence.replace(ngram,opt)\n",
    "    return transformed_sentence\n",
    "\n",
    "opinion_sentences_transformed = [transform_sentence(opinion_sentence) for opinion_sentence in opinion_sentences]\n",
    "\n",
    "opinion_sentences_transformed[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53930c4",
   "metadata": {
    "id": "b53930c4"
   },
   "source": [
    "<i>Segundo paso</i>: crear una sentence stream donde todos los tokens de las oraciones están lematizados. Los tokens no pueden ser stopwords ni tener un stopword al inicio o al final. Para simplificar la tarea, consideramos que el lema de una colocación no cambia y su PoS es 'col'. (e.g: ['The guests like scrambled eggs', 'The rooms were dirty'] -> [['the', 'guest', 'like', 'scrambled_eggs], ['the', 'room', 'be ', 'dirty']]). (1 punto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cfcef92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cfcef92",
    "outputId": "8a95c70d-0bd6-4368-e8c4-dd61ffe8bd45",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my wife', 'took me', 'my birthday', 'for breakfast', 'it was', 'excellent'], ['the weather', 'was perfect', 'sitting outside', 'overlook', 'ground', 'an absolute'], ['our waitress', 'was excellent', 'our food', 'arrived quickly', 'semi-busy', 'saturday morning'], ['pretty quickly', 'you get'], ['do yourself', 'a favor', 'get', 'their bloody', 'mary']]\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "postag = {}\n",
    "\n",
    "def get_wn_pos(pos):\n",
    "    if re.match(r'^N',pos):\n",
    "        wn_pos = 'n'\n",
    "    elif re.match(r'^V',pos):\n",
    "        wn_pos = 'v'\n",
    "    else:\n",
    "        wn_pos = 'n' #En inglés, los lemas de términos que no son verbos ni nombres se obtienen como si fueran\n",
    "                        #nombres\n",
    "    return wn_pos\n",
    "\n",
    "def wnlemmatize(t,postag):\n",
    "    lemma = \"\"\n",
    "    #Definición del lematizador\n",
    "    lem = WordNetLemmatizer()\n",
    "    #Si el candidato es monopalabra, se obtiene el lema con el lematizador de WordNet según su PoS\n",
    "    if ' ' not in t:\n",
    "        lemma = lem.lemmatize(t,get_wn_pos(postag[0][1]))\n",
    "    #Si el candidato es multipalabra, obtenemos su lema como si fuera un nombre, aplicando el lematizador de WordNet\n",
    "    else:\n",
    "        lemma = lem.lemmatize(t,'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def transform_sentence(sentence):\n",
    "    #Obtenemos los phrases según el modelo de detección de phrases que ha aprendido\n",
    "    sentence_phrases = opinion_phrases[word_tokenize(sentence.lower())]\n",
    "    #Quitamos los signos de puntuación de los phrases\n",
    "    phrases_stripped = [st.strip('\".,;:-():!?-‘’ ') for st in sentence_phrases if re.match(\"^[a-z]+.*\", st)]\n",
    "    #Quitamos stopwords\n",
    "    phrases_stripped_no_stopwords = [candidate for candidate in phrases_stripped if (len(candidate) > 0 and stopword_candidate(candidate) != True and not str(candidate) in stopwords) ]\n",
    "\n",
    "    #Hacemos etiquetaje de PoS de los phrases y lo guardamos en un diccionario (postag) \n",
    "    for ps in phrases_stripped_no_stopwords:\n",
    "        postag[ps] = nltk.pos_tag(word_tokenize(ps))\n",
    "    #Lematizamos los phrases con el lematizador de Wordnet\n",
    "    phrases_lemmatized = [wnlemmatize(ps, postag[ps]) for ps in phrases_stripped_no_stopwords]\n",
    "    #Unificamos los phrases\n",
    "    #phrases_lemmatized_and_unified = [unify(pl) for pl in phrases_lemmatized ]\n",
    "    return phrases_lemmatized\n",
    "\n",
    "transformed_sentences = [transform_sentence(ss) for ss in opinion_sentences_transformed ]\n",
    "\n",
    "print(transformed_sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b807880",
   "metadata": {
    "id": "7b807880"
   },
   "source": [
    "<i>Tercer paso</i>: Crear un modelo word2vec de las opiniones lematizadas. El modelo debe llamarse w2v_opinions (0.5 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6f1a6f2-efc8-4692-9ae9-2cc4c2fe3f97",
   "metadata": {
    "id": "b033b7ad",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2441528, 2767150)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                   #\n",
    "#############################################\n",
    "# Creamos el modelo\n",
    "w2v_opinions = gensim.models.Word2Vec(\n",
    "        transformed_sentences,\n",
    "        vector_size=150,\n",
    "        window=10, \n",
    "        min_count= 3,\n",
    "        workers= 1,\n",
    "        seed=1 # \n",
    ")\n",
    "# Entrenamos el modelo\n",
    "w2v_opinions.train(transformed_sentences, total_examples=len(transformed_sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e07b3c",
   "metadata": {
    "id": "00e07b3c"
   },
   "source": [
    "<i>Cuarto paso</i>: A partir del vocabulario del modelo word2vec, selecciona posibles aspectos de la opinión (e.g: food) y lista términos semánticamente relacionados con estos aspectos según este modelo. (0.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc470830",
   "metadata": {
    "id": "cc470830"
   },
   "source": [
    "Obtener el vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d10af9ee",
   "metadata": {
    "id": "d10af9ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocabulary = list(w2v_opinions.wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "568d1445",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "568d1445",
    "outputId": "cb7c3b6b-f658-4cdd-c3a1-b87f35d855ac",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food', 'get', 'great', 'time', 'order', 'restaurant', 'make', 'try', 'pizza', 'say', 'go', 'delicious', 'salad', 'u', 'meal', 'drink', 'fry', 'table', 'night', 'sauce', 'sandwich', 'service', 'taste', 'take', 'dish', 'tasty', 'right', 'meat', 'bread', 'thing', 'way', 'dinner', 'menu', 'day', 'amaze', 'area', 'sushi', 'enjoy', 'wait', 'hot', 'visit', 'serve', 'sweet', 'side', 'eat', 'appetizer', 'seat', 'breakfast', 'special', 'taco', 'soup', 'dessert', 'awesome', 'find', 'see', 'something', 'server', 'friend', 'spicy', 'small', 'excellent', 'perfect', 'spot', 'much', 'overall', 'bad', 'happy', 'think', 'steak', 'yummy', 'quality', 'ok', 'want', 'start', 'shrimp', 'recommend', 'best', 'salsa', 'onion', 'place', 'add', 'different', 'many', 'year', 'yes', 'disappointed', 'open', 'item', 'sure', 'guy', 'option', 'oh', 'left', 'anything', 'offer', 'great food', 'tomato', 'price', 'busy']\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                   #\n",
    "#############################################\n",
    "\n",
    "no_pos_in = ['DT', 'IN', 'PRP', 'CC', 'CD','MD', 'VBG', 'VBD', 'RP', 'RB']\n",
    "\n",
    "def good_candidate(t,postag):\n",
    "    v = False\n",
    "    #Si es multipalabra\n",
    "    if ' ' in t:\n",
    "        tl = t.split(' ') #Generamos una lista de tokens\n",
    "        #el token inicial y el token final deben ser alfabéticos y no pueden estar en la lista de stopwords..\n",
    "        if re.match(\"^[a-z]+.*\", tl[0]) and re.match(\"^[a-z]+.*\", tl[-1]) and \\\n",
    "           tl[0] not in stopwords and tl[1] not in stopwords:\n",
    "            #... ni su PoS puede estar en la lista no_pos_in\n",
    "            if postag[0][1] not in no_pos_in and postag[-1][1] not in no_pos_in:\n",
    "                v = True\n",
    "    #Si es monopalabra\n",
    "    else:\n",
    "        #debe ser alfabético, y no estar en la lista de stopwords\n",
    "        if t not in stopwords and re.match(\"^[a-z]+.*\", t):\n",
    "            #y su PoS no puede estar en la lista no_pos_in\n",
    "            if postag[0][1] not in no_pos_in:\n",
    "                v = True\n",
    "    return v\n",
    "\n",
    "def phrase_is_term(phrase):\n",
    "    test = False\n",
    "    if phrase not in postag:\n",
    "        pos = nltk.pos_tag(word_tokenize(phrase))\n",
    "    else:\n",
    "        pos = postag[phrase]\n",
    "    if good_candidate(phrase,pos):\n",
    "        test = True\n",
    "    return test\n",
    "\n",
    "terms_vocabulary = [word for word in vocabulary if phrase_is_term(word) == True]\n",
    "print(terms_vocabulary[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a6a9a07-36c8-4788-ae11-9587204279f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beer</td>\n",
       "      <td>0.807356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>list</td>\n",
       "      <td>0.805211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>special</td>\n",
       "      <td>0.786069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer</td>\n",
       "      <td>0.773176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>available</td>\n",
       "      <td>0.759095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>option</td>\n",
       "      <td>0.730790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>several</td>\n",
       "      <td>0.704829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tap</td>\n",
       "      <td>0.700200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>plate</td>\n",
       "      <td>0.684591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>drink menu</td>\n",
       "      <td>0.683819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Term  Distance\n",
       "0        beer  0.807356\n",
       "1        list  0.805211\n",
       "2     special  0.786069\n",
       "3       offer  0.773176\n",
       "4   available  0.759095\n",
       "5      option  0.730790\n",
       "6     several  0.704829\n",
       "7         tap  0.700200\n",
       "8       plate  0.684591\n",
       "9  drink menu  0.683819"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_related_terms(w2v_model, term):\n",
    "    w2v_tuples = []\n",
    "    feature_names = terms_vocabulary\n",
    "\n",
    "    for i in range(0, len(feature_names)):\n",
    "        if feature_names[i] != term and w2v_model.wv.similarity(term, feature_names[i]) > 0:\n",
    "            w2v_tuples.append((feature_names[i], w2v_model.wv.similarity(term, feature_names[i])))\n",
    "\n",
    "    #Se ordenan las tuplas\n",
    "    w2v_sorted_tuples = sorted(w2v_tuples, key=lambda tup: tup[1], reverse=True)\n",
    "    labels = ['Term', 'Distance']\n",
    "\n",
    "    #Se crea dataframe a partir del cual se construirá la tabla\n",
    "    df = pd.DataFrame.from_records(w2v_sorted_tuples, columns=labels)\n",
    "    return df\n",
    "\n",
    "df_related_terms_menu =  get_related_terms(w2v_opinions, \"menu\")\n",
    "df_related_terms_menu.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c005cc8-323e-49e9-aa78-2203e1010377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inflate</td>\n",
       "      <td>0.690379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>price</td>\n",
       "      <td>0.683504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great food</td>\n",
       "      <td>0.680267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great prices</td>\n",
       "      <td>0.636369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cheap prices</td>\n",
       "      <td>0.621569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ambiance</td>\n",
       "      <td>0.615045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good eats</td>\n",
       "      <td>0.599672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>overall</td>\n",
       "      <td>0.580139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>food quality</td>\n",
       "      <td>0.580114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pho restaurant</td>\n",
       "      <td>0.579262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Term  Distance\n",
       "0         inflate  0.690379\n",
       "1           price  0.683504\n",
       "2      great food  0.680267\n",
       "3    great prices  0.636369\n",
       "4    cheap prices  0.621569\n",
       "5        ambiance  0.615045\n",
       "6       good eats  0.599672\n",
       "7         overall  0.580139\n",
       "8    food quality  0.580114\n",
       "9  pho restaurant  0.579262"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                   #\n",
    "#############################################\n",
    "\n",
    "#Imprimimos ejemplos de aspectos cercanos semánticamente al target según el modelo Word2Vec que el alumno puede \n",
    "#encontrar en el dataframe anterior. Entre estos ejemplos está, por ejemplo, 'atmosphere' que tiene un valor de\n",
    "#similitud semántica alto. El ejercicio consiste en que el alumno elija los ejemplos que considere pertinentes.\n",
    "\n",
    "df_related_terms_menu =  get_related_terms(w2v_opinions, \"food\")\n",
    "df_related_terms_menu.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "637d9f31-5bde-4641-b96a-7c33bd5b46c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>staff</td>\n",
       "      <td>0.858470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ambiance</td>\n",
       "      <td>0.830096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great food</td>\n",
       "      <td>0.781090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scottsdalish</td>\n",
       "      <td>0.757340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ambience</td>\n",
       "      <td>0.753747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>terrible service</td>\n",
       "      <td>0.753170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>courteous</td>\n",
       "      <td>0.739609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>orient</td>\n",
       "      <td>0.736084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>soothe</td>\n",
       "      <td>0.730038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>food quality</td>\n",
       "      <td>0.727395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Term  Distance\n",
       "0             staff  0.858470\n",
       "1          ambiance  0.830096\n",
       "2        great food  0.781090\n",
       "3      scottsdalish  0.757340\n",
       "4          ambience  0.753747\n",
       "5  terrible service  0.753170\n",
       "6         courteous  0.739609\n",
       "7            orient  0.736084\n",
       "8            soothe  0.730038\n",
       "9      food quality  0.727395"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_related_terms_menu =  get_related_terms(w2v_opinions, \"service\")\n",
    "df_related_terms_menu.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca44ccde-3f9e-4daa-a645-0cc60e4ac988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>0.831953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>area</td>\n",
       "      <td>0.807573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shop</td>\n",
       "      <td>0.787037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mall</td>\n",
       "      <td>0.780538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>community</td>\n",
       "      <td>0.778575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>scottsdale</td>\n",
       "      <td>0.777343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tempe</td>\n",
       "      <td>0.767680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>din</td>\n",
       "      <td>0.756355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arena</td>\n",
       "      <td>0.744803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shopping plaza</td>\n",
       "      <td>0.743308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Term  Distance\n",
       "0        business  0.831953\n",
       "1            area  0.807573\n",
       "2            shop  0.787037\n",
       "3            mall  0.780538\n",
       "4       community  0.778575\n",
       "5      scottsdale  0.777343\n",
       "6           tempe  0.767680\n",
       "7             din  0.756355\n",
       "8           arena  0.744803\n",
       "9  shopping plaza  0.743308"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_related_terms_menu =  get_related_terms(w2v_opinions, \"restaurant\")\n",
    "df_related_terms_menu.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa3be0c0-1a8b-4c70-97bb-ed434f5553e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#help(gensim.models.Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee1a93-f92e-4e49-9742-a6616b8cc4cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "764f6cdb",
    "outputId": "45809f36-53ab-433d-8965-a854b953f0c7"
   },
   "source": [
    "<div style=\"color:blue\">Como de puede observar, hemos imprimido la lista de aspectos relacionados para los términos food, restaurant y service.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80090d4b",
   "metadata": {
    "id": "80090d4b"
   },
   "source": [
    "# 2. Detección de temas (4 puntos)\n",
    "\n",
    "En estos apartados exploraremos cúales son los tópicos tratados en las opiniones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb679a",
   "metadata": {
    "id": "00bb679a"
   },
   "source": [
    "## 2.1 Exploración de los temas con WordNet (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a2020",
   "metadata": {
    "id": "9e7a2020"
   },
   "source": [
    "En este apartado accederemos a Wordnet a través de la librería nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "320fee0c",
   "metadata": {
    "id": "320fee0c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd9034",
   "metadata": {
    "id": "7ccd9034"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Comprueba si, según Wordnet, existen aspectos que están alejados semánticamente del sentido del target, aunque en el modelo word2vec sean similares. Compruébalo calculando la similitud de Wu and Palmer entre el sentido de wordnet 'restaurant.n.01' y algunos de sus aspectos. (1 punto)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30cfcb0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30cfcb0f",
    "outputId": "edb9f0a4-45f5-4e27-f11b-64c656af9677",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA DISTANCIA SEMÁNTICA ENTRE 'RESTAURANT' Y 'MENU' ES  0.11764705882352941\n",
      "LA DISTANCIA SEMÁNTICA ENTRE 'RESTAURANT' Y 'FOOD' ES  0.3076923076923077\n",
      "LA DISTANCIA SEMÁNTICA ENTRE 'RESTAURANT' Y 'QUALITY' ES  0.16666666666666666\n",
      "LA DISTANCIA SEMÁNTICA ENTRE 'RESTAURANT' Y 'SERVICE' ES  0.125\n",
      "LA DISTANCIA SEMÁNTICA ENTRE 'RESTAURANT' Y 'BUSSINES' ES  0.13333333333333333\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                   #\n",
    "#############################################\n",
    "# definimos el sysnet de los términos a calcular\n",
    "restaurant = wn.synset('restaurant.n.01')\n",
    "menu = wn.synset('menu.n.01')\n",
    "food = wn.synset('food.n.01')\n",
    "quality = wn.synset('quality.n.01')\n",
    "service = wn.synset('service.n.01')\n",
    "business = wn.synset('business.n.01')\n",
    "\n",
    "print(\"LA DISTANCIA SEMÁNTICA ENTRE 'RESTAURANT' Y 'MENU' ES \", restaurant.wup_similarity(menu)) \n",
    "print(\"LA DISTANCIA SEMÁNTICA ENTRE 'RESTAURANT' Y 'FOOD' ES \", restaurant.wup_similarity(food)) \n",
    "print(\"LA DISTANCIA SEMÁNTICA ENTRE 'RESTAURANT' Y 'QUALITY' ES \", restaurant.wup_similarity(quality)) \n",
    "print(\"LA DISTANCIA SEMÁNTICA ENTRE 'RESTAURANT' Y 'SERVICE' ES \", restaurant.wup_similarity(service)) \n",
    "print(\"LA DISTANCIA SEMÁNTICA ENTRE 'RESTAURANT' Y 'BUSSINES' ES \", restaurant.wup_similarity(business)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22726c4-a252-4ccc-98c8-6e62de891b6c",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">Según los resultados, los términos más cercanos son <b>food</b> y <b>restaurant</b>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209e5cc",
   "metadata": {
    "id": "3209e5cc"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Lista los términos monopalabra del vocabulario de word2vec que no están en Wordnet. Los términos deben ser nombres o adjetivos. ¿Creeis que estos términos son importantes para poder realizar un buen análisis de sentimientos? (1 punto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70e8e4bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70e8e4bd",
    "outputId": "4c566845-82d9-457e-878c-eafb80b60a49",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Términos monopalabra del vocabulario de Word2Vec que no están en WordNet:\n",
      "[\"n't\", 'since', 'would', 'something', 'without', 'anything', 'scottsdale', 'could', 'pho', 'others', 'yum', 'although', 'crunchy', 'tempe', 'anyone', 'omg', 'bruschetta', 'unless', 'delish', 'gelato', 'st', 'apps', 'app', 'take-out', 'asada', 'ya', 'torta', 'hey', 'mojito', 'chris', 'yuck', 'meh', 'yelpers', 'to-go', 'feta', 'queso', 'asu', 'fyi', 'im', 'and/or', 'bianco', 'masala', 'mmmm', 'cibo', 'def', 'dr', 'um', 'alfredo', 'gallo', 'delux', 'thats', 'nigiri', 'everybody', 'definately', 'postino', 'ave', 'boba', 'ipa', 'p.s', 'kimchi', 'in-n-out', 'starbucks', 'yep', 'haha', 'lgo', 'thru', 'sub-par', 'undercooked', 'legit', 'smokey', 'fajitas', 'sakana', 'sooo', 'ahi', 'pollo', 'sandwhich', 'soooo', 'carnitas', 'quinoa', 'mmm', 'taquitos', 'burrata', 'staffperson', 'horchata', 'chimichanga', 'sans', 'fajita', 'resturant', 'francis', 'til', 'didnt', 'dang', 'via', 'gyoza', 'onto', 'hana', 'steve', 'dont', 'among', 'towards', 'pico', 'yasu', 'philly', 'hashbrowns', 'awesomeness', 'focaccia', 'wi-fi', 'hmm', 'yummm', 'pupusas', 'nutella', 'p.m', 'smashburger', 'ive', 'go-to', 'pupusa', 'yikes', 'yelper', 'empanadas', 'diego', 'definetly', 'kona', 'chimi', 'ah', 'ihop', 'high-end', 'lisa', 'rd', 'mcdonalds', 'to-die-for', 'ahwatukee', 'eggrolls', 'injera', 'poblano', 'chompies', 'sorta', 'carbs', 'mex', 'i.e', 'oj', 'sf', 'yumm', 'versus', 'jalapeño', 'mmmmmm', 'mia', 'soo', 'danielle', 'lumpia', 'whoa', 'mahi', 'tortas', 'shawarma', 'papago', 'quiznos', 'qdoba', 'underwhelmed', 'crudo', 'blanco', 'postinos', 'standouts', 'noca', 'chai', 'hiro', 'yakisoba', 'menudo', 'amongst', 'majerles', 'zipps', 'outta', 'cheesesteaks', 'hmmm', 'manchego', 'sonoran', 'dan', 'accomodating', 'coronado', 'subpar', 'az88', 'amc', 'lo', 'ciabatta', 'ginormous', 'delicous', 'over-priced', 'a+', 'aka', 'oops', 'dave', 'buca', 'portabella', 'jus', 'imho', 'u.s', 'mojitos', 'marcellino', 'los', 'anytime', 'toward', 'woohoo', 'tex-mex', 'spicey', 'habanero', 'rumbi', 'mmmmm', 'nello', 'pancetta', 'fast-food', 'alot', 'machaca', 'shea', 'kool-aid', 'goodcents', 'nobuo', 'filibertos', 'iphone', 'nypd', 'lengua', 'divey', 'alice', 'parm', 'vindaloo', 'tres', 'o.k', 'jim', 'carpaccio', 'diablo', 'anybody', 'rita', 'misto', 'arribas', 'frites', 'chambord', 'décor', 'bruchetta', 'cha', 'proscuitto', 'bandera', 'amy', 'koolaid', \"s'mores\", 'teppanyaki', 'safeway', 'sammy', 'rubios', 'rula', 'fontina', 'melty', 'yup', 'aint', 'toro', 'drive-thru', 'amanda', 'shall', 'lai', 'chimis', 'roti', 'kristi', 'oz', 'comped', 'ipad', 'ick', 'churro', 'wiseguy', 'toasty', 'churros', 'mmmmmmm', 'po-boy', 'strongbow', 'dine-in', 'luis', 'opentable', 'good-sized', 'kim', 'ticoz', 'antipasti', 'pao', 'foi', 'non-meat', 'habaneros', 'carla', 'towne', 'self-serve', 'richard', 'iceburg', 'spendy', 'somethin', 'sooooo', 'mozarella', 'malbec', 'blvd', 'nom', 'mouthwatering', 'eegees', 'lalibela', 'fogo', 'melt-in-your-mouth', 'larry', 'yo', 'well-seasoned', 'duh', 'ancho', 'reggiano', 'bulgogi', 'couldnt', 'aji', 'whats', 'yung', 'bday', 'oooh', 'isnt', 'chick-fil-a', 'artsy', 'fam', 'bento', 'hella', 'gimmicky', 'cosmo', 'uni', 'over-cooked', 'all-you-can-eat', 'sunnyslope', 'fuddruckers', 'irma', 'sinigang', 'francisco', 'arepa', 'tzatziki', 'cubano', 'margs', 'marg', 'buffett', 'sammich', 'truffled', 'wouldnt', 'calzones', 'beppo', 'everyones', 'michelin', 'atl', 'molinos', 'bingsoo', 'san', 'girly', 'brocolli', 'udupai', 'jobot', 'ayce', 'pacifico', 'distrito', 'cannoli', 'usuals', 'dk', 'baja', 'mgr', 'cenpho', 'crostini', 'yummmm', 'andre', 'portofino', 'faves', 'texting', 'marie', 'uno', 'ooooh', 'coors', 'portillo', 'in-house', 'kai', 'dissappointed', 'tmi', 'moscato', 'restaurant.com', 'whilst', 'stix', 'wongs', 'late-night', 'kathy', 'freindly', 'relleno', 'sautéed', 'margherita', 'carry-out', 'italiano', 'set-up', 'bo', 'fred', 'chimichangas', 'asahi', 'butai', 'doesnt', 'taqueria', 'restaraunt', 'coworkers', 'passionfruit', 'thin-crust', 'old-school', 'suprised', 'avacado', 'drifty', 'macayos', 'adovada', 'smashfries', 'restuarant', 'accomodated', 'soba', 'hoa', 'potstickers', 'cortadito', 'sriracha', 'sweaty', 'litchfield', 'mmmmmmmm', 'under-seasoned', 'w/o', 'chocolatey', 'thee', 'muy', 'candlelit', 'santan', 'wierd', 'amid', 'crema', 'l.a', 'umm', 'spot-on', 'macarons', 'chilaquiles', 'popeyes', 'stax', 'fundido', 'afterall', 'limoncello', 'harissa', 'chao', 'seperate', 'fri/sat', 'a++', 'grimaldis', 'onsite', 'pie-zanos', 'fresca', 'rey', 'health-conscious', 'cabeza', 'microbrews', 'vampiro', 't-bone', 'ikea', 'consistant', 'broccolini', 'orecchiette', 'out-of-towners', 'pina', 'congrats', 'arriba', 'babbo', 'spanakopita', 'bomberos', 'by-the-glass', 'teppan', 'boca', 'superbowl', 'zuchinni', 'cochinita', 'dissapointed', 'habenero', 'roka', 'angeles', 'sinfully', 'prado', 'grady', 'benihana', 'seabass', 'bcs', 'parma', 'idk', 'mcgrath', 'tableside', 'tempranillo', 'socal', 'shumai', 'vincent', 'extremly', 'wasnt', 'kabuki', 'family-owned', 'salsiccia', 'sombra', 'aways', 'kha', 'alex', 'avina', 'pepitas', 'oprah', 'someburros', 'approx', 'non-greasy', 'wayyy', 'zucca', 'chilies', 'appy', 'day/night', 'b-fast', 'coldplay', \"d'moes\", 'bianca', 'herbed', 'wy-knot', 'arepas', 'seamus', 'neds', 'jalepeno', 'foccacia', 'non-pretentious', 'taneko', 'ol', 'kick-ass', 'hmmmm', 'omfg', 'cho', 'shochu', 'fundito', 'maytag', 'vinegar-based', 'yadda', 'anywho', 'well-lit', 'maitre', 'munchies', 'double-double', 'cholula', 'i-10', 'meetup', 'chips/salsa', 'orgasmic', 'cantina', 'laura', 'theodosopoulos', 'fujiya', 'elliot', 'hoppy', 'ahhh', 'zuma', 'prosecco', 'flautas', 'goto', 'haji-baba', 'del', 'rasberry', 'taquerias', 'canoli', 'non-sushi', 'werent', 'shalt', 'alberto', 'esparza', 'scottsdalish', 'asiago', 'casa', 'cherryblossom', 'marscapone', 'marriott', 'roys', 'pan-fried', 'waaaaay', 'tony', 'definitively', 'havarti', 'crustini', 'com', 'barrumunda', 'yumminess', 'viet', 'bucco', 'bacado', 'pancit', 'gai', 'hoo', 'bertos', 'non-descript', 'salvadoreno', 'tim', 'truely', 'atleast', 'bro', 'thier', 'fest', 'mcdowell', 'wott', 'awsome', 'all-american', 'piccata', 'mini-tacos', 'service-wise', 'waaaay', 'dottsy', 'a.m', 'high-quality', 'mai', 'near-by', 'wanda', 'tiki', 'home-cooked', 'sala', 'obama', 'methinks', 'adobada', 'bros', 'teharu', 'tsoynami', 'foodgasm', 'cyclo', 'roscoes', 'giardiniera', 'tako', 'vings', 'colada', 'brioni', 'travelzoo', 'spinato', 'stay-cation', 'guero', 'empanada', 'kierland', 'marty', 'yummers', 'refried', 'fritto', 'alla', 'puttanesca', 'stateside', 'karina', 'ajillo', 'bar/restaurant', 'shouldnt', 'travis', 'dennys', 'bogi', 'rivas', 'portobello', 'convienent', 'char-grilled', 'prepped', 'betty', 'asado', 'thingies', 'spanikopita', 'promos', 'sumibiyaki', 'sara', 'applebees', 'burritoes', 'udupi', 'uhm', 'resturaunt', 'gainey', 'over-salted', 'basmati', 'tandoori', 'arent', 'sandwhiches', 'inexplicably', 'devine', 'thurs', 'jd', 'camarones', \"z'tejas\", 'umami', 'dosa', 'puri', 'chaat', 'soup/salad', 'mel', 'mexicana', 'intel', 'paso', 'pomodoro', 'precut', 'manager/owner', 'joes', 'kisra', 'swaddee', 'mid-week', 'yah', 'ahh', 'coca-cola', 'multigrain', 'scuzy', 'dennis']\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "no_wordnet_terms = []\n",
    "\n",
    "# Verificar si cada término del vocabulario está en WordNet\n",
    "for term in vocabulary:\n",
    "    #print(len(term.split(r'_')))\n",
    "    if len(term.split(r'_')) == 1 and len(term.split(r' ')) == 1 and not wn.synsets(term):\n",
    "        no_wordnet_terms.append(term)\n",
    "\n",
    "# Imprimir los términos que no están en WordNet\n",
    "print(\"Términos monopalabra del vocabulario de Word2Vec que no están en WordNet:\")\n",
    "print(no_wordnet_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6d681-364f-47e6-98e5-681d8a35deae",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">Sí, porque hay términos que pueden ser muy espécificos del ámbito culinario o expresiones que estén de moda, y que no aparezcan en la lista. Además vemos términos que son, como se comentaba en la teoría de la asignatura (2.3.1 de Extracción de sentimientos y opiniones, Joaqim Moré), acrónimos que se emplean como sinónimos de otras palabras. Por ejemplo la palabra <b>fest, mgr, fam, approx, etc.</b> Algunos otros términos que pueden ser significativos de la lista de arriba son <b>orgasmic, michelin, by-the-glass, etc. </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f788a4",
   "metadata": {
    "id": "f5f788a4"
   },
   "source": [
    "## 2.2 LDA (2 puntos)\n",
    "\n",
    "Recordad que en el notebook del módulo 1 hemos visto la aplicación del método LDA para extraer temas de documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6825ff",
   "metadata": {
    "id": "1c6825ff"
   },
   "source": [
    "<i>Primer paso</i>: Convertir las opiniones transformadas (opinion_sentences_transformed) en listas de nombres y colocaciones. Esto es necesario ya que los nombres y las colocaciones expresan los temas de las opiniones (e.g: [['wife have breakfast sitting_outside'], [' 'waitress', 'served', 'ceviche']...] -> [['wife', 'breakfast', 'sitting_outside'], ['waitress', 'ceviche']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76ad1379",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76ad1379",
    "outputId": "ed92b22a-f32b-416d-f783-58539ad3a8d9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wife', 'birthday', 'breakfast'],\n",
       " ['weather', 'which_made', 'grounds', 'pleasure'],\n",
       " ['waitress', 'food', 'morning'],\n",
       " ['looked_like', 'place', 'fills_up'],\n",
       " ['favor']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_noun_and_collocation(sentence):\n",
    "    nouns_and_collocations = []\n",
    "    noun_tags = ['NN', 'NNS']\n",
    "    tokens_pos_tagged = pos_tag(word_tokenize(sentence))\n",
    "    for tpos in tokens_pos_tagged:\n",
    "        if '_' in tpos[0]:\n",
    "            nouns_and_collocations.append(tpos[0])\n",
    "        elif tpos[1] in noun_tags:\n",
    "            nouns_and_collocations.append(tpos[0])\n",
    "    return nouns_and_collocations\n",
    "            \n",
    "noun_and_collocation_stream = [get_noun_and_collocation(opinion) for opinion in opinion_sentences_transformed]\n",
    "\n",
    "noun_and_collocation_stream[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef21d0c",
   "metadata": {
    "id": "eef21d0c"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Extrae temas a partir de las listas de nombres y colocaciones de cada oración transformada. Experimenta con el parámetro num_topics hasta encontrar un conjunto de temas informativos, asignando nombres a los temas encontrados. (2 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3284e474",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3284e474",
    "outputId": "b4dda9d2-acf2-4d04-9bc3-2d598f261dfc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                 #\n",
    "#############################################\n",
    "import gensim.corpora as corpora\n",
    "def lda(terms, num_topics):\n",
    "    dictionary = corpora.Dictionary(terms)\n",
    "    #print(dictionary)\n",
    "    # Creación del corpus\n",
    "    texts = terms\n",
    "    # Frecuencia de los términos en cada documento. El formato está en forma de tupla,\n",
    "    #(índice del término en el diccionario/vocabulario, frecuencia). Por ejemplo, [(0, 2), (1, 1), (2, 1), (3, 1)])\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    #Creación del modelo.\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, # Frecuencia de los términos en cada documento\n",
    "                                               num_topics=num_topics, #Número de temas\n",
    "                                               random_state=1, #Valor de inicio predefinido para conservar \n",
    "                                                               #coherencia\n",
    "                                               id2word = dictionary, #El vocabulario\n",
    "                                               passes=500) # Cuantos más pases, más consistente el modelo\n",
    "\n",
    "    return ldamodel\n",
    "\n",
    "ldamodel_7 = lda(noun_and_collocation_stream, 7)\n",
    "ldamodel_7.save(\"lda_7.model\")\n",
    "#ldamodel_7 = gensim.models.ldamodel.LdaModel.load(\"lda_7.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "558c738d-0a3b-44d2-a7d2-ebcf1812cad0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.027*\"time\" ', ' 0.019*\"meal\" ', ' 0.017*\"chicken\" ', ' 0.016*\"sushi\" ', ' 0.013*\"sauce\" ', ' 0.013*\"night\" ', ' 0.013*\"happy_hour\" ', ' 0.012*\"tacos\" ', ' 0.012*\"spicy\" ', ' 0.011*\"dish\"']\n",
      "['0.044*\"menu\" ', ' 0.021*\"people\" ', ' 0.020*\"day\" ', ' 0.013*\"everything\" ', ' 0.011*\"items\" ', ' 0.011*\"patio\" ', ' 0.011*\"quality\" ', ' 0.010*\"family\" ', ' 0.010*\"waiter\" ', ' 0.009*\"night\"']\n",
      "['0.024*\"staff\" ', ' 0.022*\"pizza\" ', ' 0.017*\"breakfast\" ', ' 0.012*\"friends\" ', ' 0.012*\"tables\" ', ' 0.011*\"decor\" ', ' 0.011*\"salad\" ', ' 0.011*\"visit\" ', ' 0.010*\"something\" ', ' 0.008*\"one\"']\n",
      "['0.123*\"place\" ', ' 0.044*\"food\" ', ' 0.043*\"service\" ', ' 0.015*\"experience\" ', ' 0.013*\"restaurant\" ', ' 0.011*\"atmosphere\" ', ' 0.009*\"we_were\" ', ' 0.009*\"way\" ', ' 0.008*\"places\" ', ' 0.008*\"night\"']\n",
      "['0.035*\"..\" ', ' 0.018*\"bread\" ', ' 0.015*\"sandwich\" ', ' 0.013*\"years\" ', ' 0.013*\"side\" ', ' 0.010*\"pizza\" ', ' 0.010*\"chips\" ', ' 0.009*\"things\" ', ' 0.009*\"salad\" ', ' 0.009*\"chicken\"']\n",
      "['0.092*\"food\" ', ' 0.033*\"order\" ', ' 0.026*\"bar\" ', ' 0.021*\"restaurant\" ', ' 0.020*\"times\" ', ' 0.015*\"drinks\" ', ' 0.015*\"table\" ', ' 0.015*\"time\" ', ' 0.015*\"minutes\" ', ' 0.012*\"server\"']\n",
      "['0.022*\"i\" ', ' 0.021*\"meat\" ', ' 0.021*\"burger\" ', ' 0.019*\"fries\" ', ' 0.017*\"stars\" ', ' 0.016*\"bit\" ', ' 0.014*\"taste\" ', ' 0.013*\"dishes\" ', ' 0.010*\"restaurants\" ', ' 0.010*\"shrimp\"']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"time\"</td>\n",
       "      <td>\"menu\"</td>\n",
       "      <td>\"staff\"</td>\n",
       "      <td>\"place\"</td>\n",
       "      <td>\"..\"</td>\n",
       "      <td>\"order\"</td>\n",
       "      <td>\"i\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"meal\"</td>\n",
       "      <td>\"people\"</td>\n",
       "      <td>\"pizza\"</td>\n",
       "      <td>\"food\"</td>\n",
       "      <td>\"bread\"</td>\n",
       "      <td>\"bar\"</td>\n",
       "      <td>\"meat\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"chicken\"</td>\n",
       "      <td>\"day\"</td>\n",
       "      <td>\"breakfast\"</td>\n",
       "      <td>\"service\"</td>\n",
       "      <td>\"sandwich\"</td>\n",
       "      <td>\"times\"</td>\n",
       "      <td>\"burger\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"sushi\"</td>\n",
       "      <td>\"everything\"</td>\n",
       "      <td>\"friends\"</td>\n",
       "      <td>\"experience\"</td>\n",
       "      <td>\"years\"</td>\n",
       "      <td>\"drinks\"</td>\n",
       "      <td>\"fries\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"sauce\"</td>\n",
       "      <td>\"items\"</td>\n",
       "      <td>\"tables\"</td>\n",
       "      <td>\"restaurant\"</td>\n",
       "      <td>\"side\"</td>\n",
       "      <td>\"table\"</td>\n",
       "      <td>\"stars\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"night\"</td>\n",
       "      <td>\"patio\"</td>\n",
       "      <td>\"decor\"</td>\n",
       "      <td>\"atmosphere\"</td>\n",
       "      <td>\"chips\"</td>\n",
       "      <td>\"minutes\"</td>\n",
       "      <td>\"bit\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"happy_hour\"</td>\n",
       "      <td>\"quality\"</td>\n",
       "      <td>\"salad\"</td>\n",
       "      <td>\"we_were\"</td>\n",
       "      <td>\"things\"</td>\n",
       "      <td>\"server\"</td>\n",
       "      <td>\"taste\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"tacos\"</td>\n",
       "      <td>\"family\"</td>\n",
       "      <td>\"visit\"</td>\n",
       "      <td>\"way\"</td>\n",
       "      <td>\"chicken\"</td>\n",
       "      <td></td>\n",
       "      <td>\"dishes\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"spicy\"</td>\n",
       "      <td>\"waiter\"</td>\n",
       "      <td>\"something\"</td>\n",
       "      <td>\"places\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\"restaurants\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"dish\"</td>\n",
       "      <td>\"night\"</td>\n",
       "      <td>\"one\"</td>\n",
       "      <td>\"night\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\"shrimp\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               1              2             3              4            5           6               7\n",
       "0        \"time\"         \"menu\"       \"staff\"        \"place\"         \"..\"     \"order\"             \"i\" \n",
       "1        \"meal\"       \"people\"       \"pizza\"         \"food\"      \"bread\"       \"bar\"          \"meat\" \n",
       "2     \"chicken\"          \"day\"   \"breakfast\"      \"service\"   \"sandwich\"     \"times\"        \"burger\" \n",
       "3       \"sushi\"   \"everything\"     \"friends\"   \"experience\"      \"years\"    \"drinks\"         \"fries\" \n",
       "4       \"sauce\"        \"items\"      \"tables\"   \"restaurant\"       \"side\"     \"table\"         \"stars\" \n",
       "5       \"night\"        \"patio\"       \"decor\"   \"atmosphere\"      \"chips\"   \"minutes\"           \"bit\" \n",
       "6  \"happy_hour\"      \"quality\"       \"salad\"      \"we_were\"     \"things\"     \"server\"        \"taste\" \n",
       "7       \"tacos\"       \"family\"       \"visit\"          \"way\"     \"chicken\"                   \"dishes\" \n",
       "8       \"spicy\"       \"waiter\"   \"something\"       \"places\"                            \"restaurants\" \n",
       "9         \"dish\"        \"night\"         \"one\"        \"night\"                                 \"shrimp\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "K = ldamodel_7.num_topics\n",
    "topicWordProbMat = ldamodel_7.print_topics(num_topics=K, num_words=10)\n",
    "\n",
    "#La matriz se muestra como un dataframe\n",
    "columns = ['1','2','3', '4', '5','6','7']\n",
    "df_topic = pd.DataFrame(columns = columns)\n",
    "pd.set_option('display.width', 2000)\n",
    "\n",
    "zz = np.zeros(shape=(1000,K))\n",
    "\n",
    "last_number=0\n",
    "DC={}\n",
    "\n",
    "for x in range (10):\n",
    "    data = pd.DataFrame({columns[0]:\"\",\n",
    "                     columns[1]:\"\",\n",
    "                     columns[2]:\"\",\n",
    "                     columns[3]:\"\",\n",
    "                     columns[4]:\"\",                                                                               \n",
    "                     columns[5]:\"\",                                                                               \n",
    "                     columns[6]:\"\",\n",
    "                            },index=[0])\n",
    "    df_topic= pd.concat([df_topic, data], ignore_index=True)\n",
    "\n",
    "\n",
    "for line in topicWordProbMat:\n",
    "    topic, word = line\n",
    "    probabilities=word.split(\"+\")\n",
    "    print(probabilities)\n",
    "    y=0\n",
    "    for pr in probabilities:   \n",
    "        a=pr.split(\"*\")\n",
    "        df_topic.iloc[y,topic] = a[1]\n",
    "        if a[1] in DC:\n",
    "                zz[DC[a[1]]][topic]=a[0]\n",
    "        else:\n",
    "            zz[last_number][topic]=a[0]\n",
    "            DC[a[1]]=last_number\n",
    "            last_number=last_number+1\n",
    "            y=y+1\n",
    "\n",
    "#Ordenar strings con números          \n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    def atoi(text):\n",
    "        return int(text) if text.isdigit() else text\n",
    "\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]\n",
    "\n",
    "df_topic =df_topic.reindex(columns=sorted(df_topic.columns, key=natural_keys))\n",
    "df_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18bec559-f690-463f-a564-dd67aa0cf894",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic with num_topics = 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('menu', 0.04371323),\n",
       " ('people', 0.02121672),\n",
       " ('day', 0.020214884),\n",
       " ('everything', 0.013295376),\n",
       " ('items', 0.011493003),\n",
       " ('patio', 0.011377151),\n",
       " ('quality', 0.011224762),\n",
       " ('family', 0.010043187),\n",
       " ('waiter', 0.00957674),\n",
       " ('night', 0.009346394)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Topic with num_topics = 7\")\n",
    "ldamodel_7.show_topic(1, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd9a95-8342-497b-90f2-e3b392be83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_4 = lda(noun_and_collocation_stream, 4)\n",
    "ldamodel_4.save(\"lda_4.model\")\n",
    "#ldamodel_4 = gensim.models.ldamodel.LdaModel.load(\"lda_4.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a1c9e63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a1c9e63",
    "outputId": "ed6fdc3c-a83e-465c-ab5c-15be577cd159",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.024*\"time\" ', ' 0.016*\"salad\" ', ' 0.015*\"chicken\" ', ' 0.012*\"order\" ', ' 0.011*\"sauce\" ', ' 0.011*\"side\" ', ' 0.010*\"i\" ', ' 0.008*\"sandwich\" ', ' 0.007*\"meat\" ', ' 0.007*\"tacos\"']\n",
      "['0.025*\"menu\" ', ' 0.011*\"bread\" ', ' 0.009*\"cheese\" ', ' 0.009*\"dishes\" ', ' 0.008*\"happy_hour\" ', ' 0.008*\"everything\" ', ' 0.008*\"burger\" ', ' 0.007*\"drink\" ', ' 0.007*\"wine\" ', ' 0.007*\"beer\"']\n",
      "['0.021*\"place\" ', ' 0.020*\"..\" ', ' 0.017*\"pizza\" ', ' 0.017*\"bar\" ', ' 0.012*\"staff\" ', ' 0.012*\"people\" ', ' 0.010*\"night\" ', ' 0.010*\"table\" ', ' 0.010*\"minutes\" ', ' 0.009*\"order\"']\n",
      "['0.089*\"food\" ', ' 0.058*\"place\" ', ' 0.028*\"service\" ', ' 0.019*\"restaurant\" ', ' 0.010*\"experience\" ', ' 0.009*\"sushi\" ', ' 0.008*\"stars\" ', ' 0.008*\"things\" ', ' 0.008*\"bit\" ', ' 0.008*\"years\"']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"time\"</td>\n",
       "      <td>\"menu\"</td>\n",
       "      <td>\"place\"</td>\n",
       "      <td>\"food\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"salad\"</td>\n",
       "      <td>\"bread\"</td>\n",
       "      <td>\"..\"</td>\n",
       "      <td>\"service\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"chicken\"</td>\n",
       "      <td>\"cheese\"</td>\n",
       "      <td>\"pizza\"</td>\n",
       "      <td>\"restaurant\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"order\"</td>\n",
       "      <td>\"dishes\"</td>\n",
       "      <td>\"bar\"</td>\n",
       "      <td>\"experience\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"sauce\"</td>\n",
       "      <td>\"happy_hour\"</td>\n",
       "      <td>\"staff\"</td>\n",
       "      <td>\"sushi\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"side\"</td>\n",
       "      <td>\"everything\"</td>\n",
       "      <td>\"people\"</td>\n",
       "      <td>\"stars\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"i\"</td>\n",
       "      <td>\"burger\"</td>\n",
       "      <td>\"night\"</td>\n",
       "      <td>\"things\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"sandwich\"</td>\n",
       "      <td>\"drink\"</td>\n",
       "      <td>\"table\"</td>\n",
       "      <td>\"bit\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"meat\"</td>\n",
       "      <td>\"wine\"</td>\n",
       "      <td>\"minutes\"</td>\n",
       "      <td>\"years\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"tacos\"</td>\n",
       "      <td>\"beer\"</td>\n",
       "      <td>\"order\"</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             1              2           3              4\n",
       "0      \"time\"         \"menu\"     \"place\"         \"food\" \n",
       "1     \"salad\"        \"bread\"        \"..\"      \"service\" \n",
       "2   \"chicken\"       \"cheese\"     \"pizza\"   \"restaurant\" \n",
       "3     \"order\"       \"dishes\"       \"bar\"   \"experience\" \n",
       "4     \"sauce\"   \"happy_hour\"     \"staff\"        \"sushi\" \n",
       "5      \"side\"   \"everything\"    \"people\"        \"stars\" \n",
       "6         \"i\"       \"burger\"     \"night\"       \"things\" \n",
       "7  \"sandwich\"        \"drink\"     \"table\"          \"bit\" \n",
       "8      \"meat\"         \"wine\"   \"minutes\"         \"years\"\n",
       "9      \"tacos\"         \"beer\"     \"order\"               "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = ldamodel_4.num_topics\n",
    "topicWordProbMat = ldamodel_4.print_topics(num_topics=K, num_words=10)\n",
    "\n",
    "#La matriz se muestra como un dataframe\n",
    "columns = ['1','2','3', '4']\n",
    "df_topic = pd.DataFrame(columns = columns)\n",
    "pd.set_option('display.width', 2000)\n",
    "\n",
    "zz = np.zeros(shape=(1000,K))\n",
    "\n",
    "last_number=0\n",
    "DC={}\n",
    "\n",
    "for x in range (10):\n",
    "    data = pd.DataFrame({columns[0]:\"\",\n",
    "                     columns[1]:\"\",\n",
    "                     columns[2]:\"\",\n",
    "                     columns[3]:\"\"\n",
    "                            },index=[0])\n",
    "    df_topic= pd.concat([df_topic, data], ignore_index=True)\n",
    "\n",
    "\n",
    "for line in topicWordProbMat:\n",
    "    topic, word = line\n",
    "    probabilities=word.split(\"+\")\n",
    "    print(probabilities)\n",
    "    y=0\n",
    "    for pr in probabilities:   \n",
    "        a=pr.split(\"*\")\n",
    "        df_topic.iloc[y,topic] = a[1]\n",
    "        if a[1] in DC:\n",
    "                zz[DC[a[1]]][topic]=a[0]\n",
    "        else:\n",
    "            zz[last_number][topic]=a[0]\n",
    "            DC[a[1]]=last_number\n",
    "            last_number=last_number+1\n",
    "            y=y+1\n",
    "\n",
    "#Ordenar strings con números          \n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    def atoi(text):\n",
    "        return int(text) if text.isdigit() else text\n",
    "\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]\n",
    "\n",
    "df_topic = df_topic.reindex(columns=sorted(df_topic.columns, key=natural_keys))\n",
    "df_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c007e2d-149c-4c7a-ba39-6199214fd85b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic with num_topics = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('menu', 0.025092797),\n",
       " ('bread', 0.010584909),\n",
       " ('cheese', 0.008714047),\n",
       " ('dishes', 0.008699143),\n",
       " ('happy_hour', 0.0077446573),\n",
       " ('everything', 0.0076329987),\n",
       " ('burger', 0.007539459),\n",
       " ('drink', 0.007326105),\n",
       " ('wine', 0.0072688516),\n",
       " ('beer', 0.007145796)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Topic with num_topics = 4\")\n",
    "ldamodel_4.show_topic(1, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd44d58-9fd5-4eb8-84d4-ecf767cbd702",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">Viendo los resultados, seleccionaré el modelo con num_topics = 7 </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365d758-5ed9-4902-86e8-c8db11bbe7b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong> Ejercicio opcional:</strong> Si consideramos también los verbos, ¿mejoraríamos los resultados? Justifica la respuesta mostrando LDA si consideramos los verbos, además de los nombres y colocaciones.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "724d0488-15d9-426f-bc63-103f5e297dcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a1c9e63",
    "outputId": "ed6fdc3c-a83e-465c-ab5c-15be577cd159",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wife', 'took', 'birthday', 'breakfast', 'was'],\n",
       " ['weather',\n",
       "  'was',\n",
       "  'which_made',\n",
       "  'sitting',\n",
       "  'overlooking',\n",
       "  'grounds',\n",
       "  'pleasure'],\n",
       " ['waitress', 'was', 'food', 'arrived', 'morning'],\n",
       " ['looked_like', 'place', 'fills_up'],\n",
       " ['favor']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                 #\n",
    "#############################################\n",
    "def get_noun_verbs_and_collocation(sentence):\n",
    "    nouns_and_collocations = []\n",
    "    noun_tags = ['NN', 'NNS', 'VBD', 'VBG', 'VM' ]\n",
    "    tokens_pos_tagged = pos_tag(word_tokenize(sentence))\n",
    "    for tpos in tokens_pos_tagged:\n",
    "        if '_' in tpos[0]:\n",
    "            nouns_and_collocations.append(tpos[0])\n",
    "        elif tpos[1] in noun_tags:\n",
    "            nouns_and_collocations.append(tpos[0])\n",
    "    return nouns_and_collocations\n",
    "            \n",
    "noun_verb_and_collocation_stream = [get_noun_verbs_and_collocation(opinion) for opinion in opinion_sentences_transformed]\n",
    "\n",
    "noun_verb_and_collocation_stream[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ccebd7-84d0-4a4b-a5b7-cd5958a20d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ldamodel_new = lda(noun_verb_and_collocation_stream, 7)\n",
    "ldamodel_new.save(\"lda_verb.model\")\n",
    "#ldamodel_new = gensim.models.ldamodel.LdaModel.load(\"lda_verb.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80c13bac-8a91-4962-9e64-e7db7d4ca277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo modelo con verbos y num_topics = 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('was', 0.029652366),\n",
       " ('times', 0.020628674),\n",
       " ('place', 0.019048363),\n",
       " ('had', 0.018653965),\n",
       " ('go_back', 0.009388573),\n",
       " ('price', 0.009331025),\n",
       " ('wait', 0.0092755575),\n",
       " ('decor', 0.009095241),\n",
       " ('would_be', 0.008546606),\n",
       " ('first_time', 0.007924653)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Nuevo modelo con verbos y num_topics = 7\")\n",
    "ldamodel_new.show_topic(1, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "FSnFlmpKE9UF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSnFlmpKE9UF",
    "outputId": "b52a4f5b-2491-4e17-8bcd-1b2deaf28b23",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pyLDAvis) (1.24.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pyLDAvis) (1.10.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pyLDAvis) (2.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: numexpr in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pyLDAvis) (2.8.4)\n",
      "Requirement already satisfied: funcy in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pyLDAvis) (1.2.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pyLDAvis) (56.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.1.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fpa\\downloads\\uoc\\text_mining\\pla1-analisis\\notebook-es\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    " !pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aeb8277",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 882
    },
    "id": "0aeb8277",
    "outputId": "96f81128-b26e-4f87-88e6-a694d4322440",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el24780132965858884855847118\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el24780132965858884855847118_data = {\"mdsDat\": {\"x\": [0.13488532061635974, -0.29126175704447355, -0.08776493494165262, 0.1563858054208885, -0.16082494380756582, 0.19157643419016848, 0.057004075566275375], \"y\": [0.05631715001676851, -0.13821143213184167, -0.113504517770982, -0.1347789536825321, 0.18894595863749872, -0.10700434823473846, 0.24823614316582693], \"topics\": [1, 2, 3, 4, 5, 6, 7], \"cluster\": [1, 1, 1, 1, 1, 1, 1], \"Freq\": [18.724603665652477, 15.677213707703544, 14.25253799636749, 13.912512227346774, 13.37719668346712, 12.201221095023145, 11.854714624439444]}, \"tinfo\": {\"Term\": [\"looked_like\", \"garden\", \"ordered\", \"sitting\", \"server\", \"tasting\", \"bunch_of\", \"salsa\", \"comeback\", \"one\", \"order\", \"tomato\", \"how_long\", \"by_mistake\", \"wooden\", \"girls\", \"aioli\", \"even_know_how_much\", \"EVERYTHING\", \"bread\", \"were_awesome\", \"mass\", \"saying\", \"helped\", \"thought\", \"disappointment\", \"candy\", \"placed_our\", \"monotone\", \"rings\", \"ordered\", \"mass\", \"thought\", \"placed_our\", \"seat\", \"reviewers_have_some\", \"tip\", \"hamburger_bun\", \"chopped\", \"winner\", \"favor\", \"carafe\", \"employees\", \"dumplings\", \"guy\", \"entrees_before_we\", \"crisp\", \"costs\", \"including\", \"yummy\", \"romaine_lettuce\", \"fills_up\", \"patio\", \"glass_of\", \"options\", \"appetizer\", \"prices\", \"thyme\", \"be_going_back\", \"host\", \"were_awesome\", \"sitting\", \"situation\", \"were\", \"wanted\", \"go_back\", \"order\", \"idea\", \"knew\", \"server\", \"tomato\", \"EVERYTHING\", \"bread\", \"aioli\", \"everything_on\", \"bunch_of\", \"salsa\", \"girls\", \"saying\", \"candy\", \"called\", \"uprooted\", \"providing\", \"powdery\", \"Followed\", \"live_music\", \"eating\", \"disappoint\", \"peeves\", \"race\", \"Sitting\", \"pho\", \"claiming\", \"topping\", \"food\", \"app\", \"working\", \"beer_before\", \"servers\", \"grounds\", \"love_having\", \"two-thirds\", \"have_never_made\", \"bacon\", \"artifacts\", \"tart\", \"griping\", \"by_myself\", \"wooden\", \"bucks_for_dinner\", \"beverage\", \"have_been_here\", \"garden\", \"door\", \"tacos\", \"person\", \"cold\", \"by_mistake\", \"please_everyone\", \"steep\", \"cinammoness\", \"joy\", \"Had\", \"have_been_coming_here_for\", \"menu\", \"six-pack\", \"building\", \"soggy\", \"go_wrong\", \"tiramisu\", \"others\", \"point\", \"drums\", \"Farm\", \"fried_steak\", \"yelp\", \"place\", \"waited\", \"comfort\", \"thumbs\", \"events/pool\", \"Management\", \"maitre\", \"choice_for\", \"group\", \"been_driving\", \"mom\", \"was_outstanding\", \"orders\", \"part_of\", \"sun\", \"garden\", \"order\", \"sitting\", \"love_their\", \"butter\", \"restaurants\", \"wooden\", \"spot\", \"one\", \"choices\", \"pleasure\", \"things\", \"curry\", \"great_experience\", \"refill_our\", \"moseyed\", \"went_outside\", \"tofu\", \"flavors\", \"drink\", \"rate\", \"goods\", \"basketball_game\", \"finished\", \"bombs\", \"radishes\", \"plan_on\", \"get_your\", \"come_here\", \"dishes\", \"pesto\", \"mall\", \"veggies_were\", \"better\", \"size\", \"received\", \"rice\", \"every_time\", \"pieces_of\", \"let_me\", \"EVERYTHING\", \"order\", \"wind_up\", \"sausage\", \"sitting\", \"left\", \"way\", \"aioli\", \"issues\", \"even_know_how_much\", \"idea\", \"leaves\", \"tasting\", \"good_lighting\", \"overlooking\", \"bill\", \"summer\", \"2_people\", \"weekday\", \"boss\", \"great_place\", \"something\", \"hot_sauce\", \"being\", \"recommended_over\", \"places_near_the\", \"Something\", \"list\", \"brought\", \"standing\", \"thoughts\", \"sounding\", \"par\", \"would_be\", \"selling\", \"breakfast/brunch_place\", \"went_with\", \"weeks\", \"carbonara\", \"produced\", \"marg\", \"perfect_for_me\", \"garden\", \"tomato\", \"looked_like\", \"Drop\", \"construction\", \"dish\", \"sitting\", \"leaves\", \"wooden\", \"horchata\", \"spaghetti\", \"bread\", \"griping\", \"everything\", \"pizza\", \"how_long\", \"helped\", \"disappointment\", \"rings\", \"mozzarella\", \"mmmm\", \"fixin\", \"but_none\", \"kick\", \"sandwich\", \"burgers\", \"Yum\", \"Oprah\", \"seasoned\", \"coconut_milk\", \"splitting\", \"cheese\", \"favorites\", \"course\", \"mushrooms\", \"love_love\", \"finished_off\", \"fiance\", \"tap\", \"tazer\", \"scrambled\", \"pretty_large\", \"her_fiance\", \"white_truffle\", \"reviews\", \"even_know_how_much\", \"entrees_are\", \"gave\", \"issues\", \"aioli\", \"order\", \"sitting\", \"server\", \"were_awesome\", \"comeback\", \"monotone\", \"girl\", \"get_some\", \"whether_or\", \"ever_eaten\", \"charging\", \"aspect\", \"cards_are\", \"melts\", \"problem\", \"can_choose\", \"sides\", \"court\", \"ages\", \"have_gone_after\", \"freebies\", \"Things\", \"jar_of\", \"deli\", \"had_never_been_there\", \"whisked\", \"was_hoping_for\", \"word\", \"talents\", \"were_planning_on\", \"was_still\", \"recalling\", \"choice\", \"people\", \"looked_like\", \"server\", \"bread\", \"pancakes\", \"blah_blah_blah\", \"sitting\", \"waiter_came\", \"even_if\", \"value\", \"bites\", \"blue_cheese\", \"left_super\", \"order\", \"Drop\", \"everything_on\", \"hey\"], \"Freq\": [3302.0, 3441.0, 1539.0, 10491.0, 1967.0, 1059.0, 1011.0, 936.0, 759.0, 775.0, 3624.0, 1037.0, 613.0, 652.0, 1304.0, 604.0, 1023.0, 680.0, 1016.0, 966.0, 854.0, 601.0, 533.0, 463.0, 562.0, 422.0, 458.0, 498.0, 389.0, 387.0, 1538.9593852011044, 600.3987110287692, 561.6478271992135, 497.67589465414426, 433.8681951794466, 386.9787448346074, 360.50717362564507, 354.2501527359799, 337.52382466292727, 284.1992183592761, 270.1580835851318, 234.71937513276796, 217.58638942655597, 215.6694604104526, 212.15173267028442, 210.201779156395, 183.52490694187512, 166.75579010207744, 160.75273688127632, 157.21893193390144, 154.78662353343788, 148.15696527048138, 141.79993313093428, 129.86728106116414, 122.83770347738925, 121.86656475487693, 119.13020088606764, 118.06666897698001, 112.91127998052862, 110.67486601760568, 633.9336963971069, 4675.699349264178, 357.71822771835264, 285.7201047593656, 204.05883586446853, 261.41448042774, 926.0506154413326, 330.27166041725957, 214.13318320226136, 509.9342866034235, 368.9982979234993, 301.80949313331513, 268.8804144904584, 245.27859093132236, 206.48812797590713, 1011.038635351994, 936.0988541274901, 603.803325919034, 532.4449423554448, 457.84770820597953, 388.762147149067, 356.9341493208427, 356.78155722588457, 290.6816921631914, 256.2395712619842, 196.26555195157397, 194.25762064474878, 194.32104689427626, 191.0899181871712, 188.21572806501547, 181.82525012768255, 158.7474198325349, 154.87220225612154, 150.6289408351428, 136.88336527426102, 134.85616164735862, 130.70217129657365, 129.39975583808484, 123.96675849150037, 109.55171061075924, 107.23561644380001, 103.10118980474398, 95.61483643051398, 94.83900615777162, 94.57056969282402, 377.619071371949, 426.27367439066677, 270.5741141047212, 780.5520478207536, 249.24338721124644, 277.690772415026, 243.04090545455736, 866.7579368531078, 164.61107785195745, 206.4609411351268, 159.0777932119011, 156.08625319622197, 651.9722427426454, 296.72720145901815, 294.9084089852334, 293.15534394091185, 287.45642553488364, 270.11671724792785, 250.45980065059953, 200.0880559547489, 197.3742607563904, 179.58669053347515, 176.54943867485187, 175.9309885985189, 156.55767423266303, 155.0220277542752, 148.21427907717282, 145.54369385035574, 142.17260964007235, 133.01954262342704, 132.81386845711526, 129.05586662018882, 122.42070027662884, 121.59728982268291, 120.50642936606786, 118.90025026213091, 116.49768901240056, 115.01850542768395, 111.11511881423436, 110.53914284480484, 106.54141102612151, 101.394089394067, 187.82953714483355, 209.3569186308247, 157.10115441967994, 194.45667520519333, 602.0262840436153, 589.5612668989936, 937.1673562450565, 142.43098643175531, 203.88420501450756, 147.8521012196582, 218.41878258424038, 133.46459355646948, 774.9292923599189, 398.24810144333, 344.665786961616, 338.2921166127597, 269.8940476788473, 211.64496606839032, 206.2634671563233, 188.11155546268566, 171.92009568655197, 168.15367137690197, 162.78162541077424, 159.2590669937078, 135.59939203867074, 128.8892945043934, 115.10098764210592, 115.09131202038839, 114.5061990892491, 114.09733044501498, 112.75608958507426, 112.23462601105376, 107.07236936035449, 105.84199230560185, 103.11594514717788, 102.63157504795272, 100.61553319988307, 98.25841670740857, 93.85256094291627, 93.05719179552328, 87.7597637643634, 87.12951229805489, 335.6849136720911, 245.73234659030027, 605.838328350921, 1146.6770511287289, 170.46777552873462, 134.7040342837737, 1367.0240142152547, 141.62646394720477, 151.77497718674522, 216.55459345717017, 165.64671563496375, 161.43871807404182, 151.85304300473044, 124.66466090791262, 1059.0023692620039, 352.9882932243886, 322.2958956018653, 317.04326439583605, 297.06222523012525, 287.85528501956964, 285.72735717089296, 267.0354404770734, 237.75289451083472, 200.42485347740214, 197.73433315481537, 186.18367203853572, 174.5746078339706, 169.73921061130545, 144.1436289545417, 138.6596093540303, 135.87848688951715, 125.85956694017288, 117.0049791364258, 114.32570983451275, 109.18209404304099, 107.79618792922156, 105.19010896867104, 103.6797578637532, 96.76959369242718, 95.72655708094422, 91.57272030708485, 91.54440282104292, 85.8763886282981, 83.78265587069303, 1972.043430860546, 668.2257031884034, 1438.8447837301221, 260.96442011102454, 275.2511029152676, 138.17373654718207, 1061.817099747756, 186.7522457180342, 304.71203523016254, 130.4074657057694, 142.04559332039904, 181.9251089132486, 160.4397343817279, 141.33503162186398, 139.50405060120846, 612.9394554215304, 462.3226311085476, 421.97005371961814, 387.1345150191164, 345.10832552670934, 315.6006376488023, 262.2550266942045, 259.2881688744033, 235.97608122448491, 203.20287420821148, 203.41157805727428, 200.2881136063925, 199.99969534422857, 187.11822667946285, 173.59601892940054, 148.38288048700477, 140.79527031252033, 138.44246671521375, 128.4090212970913, 121.32151921706496, 120.47833802813173, 119.76423446875773, 113.54071466760138, 107.08486773232381, 105.11377862430524, 104.94495717996374, 101.8816155644413, 101.22017618189086, 100.29408041217393, 99.34063832767336, 518.7355901878138, 326.1519936690029, 223.77283392397516, 329.6624264277697, 561.6220704270792, 685.1331216942123, 1086.3148105532732, 354.52200933666705, 220.07961363474033, 758.6940555958244, 388.5125881301452, 336.6419766178908, 309.98389673628867, 235.11338940274203, 199.8763880415445, 186.9110732479258, 150.750381635628, 137.03497017998433, 125.68693655185876, 104.14672765970424, 103.81502570052135, 101.44661348485879, 92.44534948968051, 91.17800979029663, 80.67058484715793, 76.45209904773526, 68.10442758514999, 66.78914491336549, 63.39951617030602, 63.35988502690068, 63.234460873902634, 62.00961677740111, 61.77358654062269, 59.28595650082896, 57.01680270255705, 56.94382631293204, 56.81965689111517, 56.521232851112956, 56.07978781837672, 1862.921443473887, 1102.4683649335764, 515.1937981525251, 258.2640488262522, 193.21818457152918, 1362.8928772346112, 100.66901678772643, 109.16078035502963, 107.02231446007677, 112.64420790468166, 81.38948460757102, 98.78488184287741, 276.80398124182506, 115.85190062647104, 93.11396207260815, 86.81977141985091], \"Total\": [3302.0, 3441.0, 1539.0, 10491.0, 1967.0, 1059.0, 1011.0, 936.0, 759.0, 775.0, 3624.0, 1037.0, 613.0, 652.0, 1304.0, 604.0, 1023.0, 680.0, 1016.0, 966.0, 854.0, 601.0, 533.0, 463.0, 562.0, 422.0, 458.0, 498.0, 389.0, 387.0, 1539.7406413339143, 601.1800890243032, 562.4289777473906, 498.45700069965875, 434.64975386988584, 387.7602692523908, 361.2885134260096, 355.0312642300548, 338.30565446454256, 284.98078368584993, 270.9388980972269, 235.50085426711757, 218.367321138605, 216.45137276629748, 212.93288286043108, 210.98327671538632, 184.3064083919564, 167.53782295066762, 161.533746294613, 158.0009913073763, 155.56776832503843, 148.93803186244335, 142.58209538963231, 130.64852017382663, 123.61935523480389, 122.64813433444652, 119.91146466442571, 118.84795787401724, 113.69261073518886, 111.45599834088186, 854.6760417802864, 10491.079330122424, 461.4119925387906, 354.8027063055819, 246.26464440679555, 380.1847148262289, 3624.500908150797, 694.9031488318303, 312.23976970492095, 1967.4803321252687, 1037.8949390919975, 1016.5299909116525, 966.5632867898769, 1023.9843956754775, 378.8418568601764, 1011.797899197847, 936.8580234115133, 604.5621359390162, 533.2040716126716, 458.60693302007076, 389.52127089180783, 357.69337296057705, 357.54055836244044, 291.44148450202044, 256.9983710421176, 197.02483205991956, 195.01658995363996, 195.08067402019955, 191.84877473494984, 188.9743537586318, 182.58433033540064, 159.50650613722325, 155.63169052238817, 151.38910486504213, 137.64293623863648, 135.6158548630709, 131.46202824836251, 130.15907956120097, 124.72637207094016, 110.31083058309622, 107.99500440084158, 103.8617956056419, 96.37359307837076, 95.59853205365033, 95.32913591924947, 488.7691407269222, 587.362293307551, 340.19931641392884, 1304.1841581547874, 315.52681177570565, 362.81978838769356, 307.4936627294658, 3441.328713096183, 206.736379312716, 513.6396530834077, 227.70242255468963, 287.50553146927825, 652.7472228974367, 297.50221761813646, 295.6838734538893, 293.93022068682666, 288.231932063741, 270.89238640950947, 251.23517711162458, 200.86357485264347, 198.14987473472303, 180.36188718009836, 177.32469155984424, 176.70655232606205, 157.33354061744282, 155.79731243154208, 148.99006507006288, 146.3193547829451, 142.94810655571382, 133.79488361202309, 133.5895963527637, 129.83122518283008, 123.19612934043657, 122.37299359568213, 121.28195581548493, 119.67535614033117, 117.2731624859247, 115.79426643339359, 111.8906572016025, 111.31441929880535, 107.31643172069779, 102.16927727022158, 215.88410621485633, 319.20254113881106, 205.0512211994698, 302.76721450575036, 3441.328713096183, 3624.500908150797, 10491.079330122424, 194.47735541805065, 661.6448502352326, 254.3948456646611, 1304.1841581547874, 181.01634540408924, 775.7185278060432, 399.0374873405474, 345.4554163943757, 339.08174713427223, 270.68400157715024, 212.4347383162492, 207.05285810735242, 188.9014883503901, 172.70992229865928, 168.94361006783296, 163.57048914150545, 160.04954491400366, 136.38916827841447, 129.67909613652526, 115.89029246249706, 115.88152708281231, 115.29593766567966, 114.88675727520224, 113.54576179630877, 113.02441378486402, 107.86207870008505, 106.63125561329964, 103.90528282285908, 103.42066743481607, 101.40512065372593, 99.04811589977157, 94.64274646308525, 93.84758183624153, 88.54917349386702, 87.9188864333266, 456.1924702006767, 323.84130201604916, 1016.5299909116525, 3624.500908150797, 228.86269431702755, 164.11333481378634, 10491.079330122424, 189.26637016528744, 323.14719224333555, 1023.9843956754775, 495.979513503158, 680.8446722505369, 694.9031488318303, 388.1563327831789, 1059.8139343913685, 353.7999115656693, 323.1078281618761, 317.8547640741425, 297.87386465663013, 288.66701448106295, 286.5393898500707, 267.8469515604216, 238.56462411413133, 201.23655902666724, 198.54579721582857, 186.99546039048656, 175.38693826527276, 170.55125246143402, 144.95544905132337, 139.47101689881728, 136.69049418106658, 126.67141742737779, 117.81723211583737, 115.13811977568086, 109.99331986010185, 108.60807188325033, 106.00194529695902, 104.49179845887133, 97.58139026690013, 96.5385180830414, 92.38428675821685, 92.35625220541772, 86.68871904042061, 84.59463490202366, 3441.328713096183, 1037.8949390919975, 3302.4713247815407, 377.5217473624014, 489.52106792449393, 186.84502643942645, 10491.079330122424, 388.1563327831789, 1304.1841581547874, 178.01304655466865, 264.69206133135236, 966.5632867898769, 587.362293307551, 295.3383152343066, 306.56560095311005, 613.7426103664689, 463.126301556468, 422.7733045338331, 387.9377012444965, 345.91148706459916, 316.40414225085794, 263.05826631278444, 260.0916877480613, 236.77925879567638, 204.00601177198885, 204.21571009746302, 201.09138565583837, 200.80269868767945, 187.92128828836786, 174.39937367745657, 149.18595559997, 141.5981200586072, 139.24614597484927, 129.21240492460183, 122.1250686204234, 121.28207898073215, 120.56732251880102, 114.34400122325248, 107.88786503687481, 105.91743107598404, 105.74869978148419, 102.68646648666208, 102.0233646805833, 101.09724942926539, 100.14394041307182, 680.8446722505369, 436.1049543301711, 292.4242234836738, 495.979513503158, 1023.9843956754775, 3624.500908150797, 10491.079330122424, 1967.4803321252687, 854.6760417802864, 759.5094732816119, 389.3284821815952, 337.45773207375817, 310.7999493849558, 235.92905742272652, 200.69218504226586, 187.7268284552903, 151.56543339431184, 137.85036719204368, 126.50235155351346, 104.96270401110283, 104.63125368953826, 102.26193111132038, 93.26162857966233, 91.99378134938276, 81.48660608625161, 77.2675122757218, 68.92027199590676, 67.60537427104923, 64.21544323577896, 64.17547285564248, 64.05123064914116, 62.82535805191436, 62.58912007916742, 60.10136210757562, 57.833171534903634, 57.7598205433348, 57.635419921399624, 57.33728824191583, 56.89603273660127, 3302.4713247815407, 1967.4803321252687, 966.5632867898769, 491.2094884629569, 357.69361596996384, 10491.079330122424, 144.44399421180424, 168.26568012409817, 170.3347424331047, 191.8958495807549, 105.65245838343047, 159.32715881718573, 3624.500908150797, 377.5217473624014, 378.8418568601764, 258.11201923856964], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.2951, -4.2364, -4.3031, -4.424, -4.5612, -4.6756, -4.7465, -4.764, -4.8123, -4.9843, -5.035, -5.1756, -5.2514, -5.2602, -5.2767, -5.2859, -5.4216, -5.5174, -5.5541, -5.5763, -5.5919, -5.6357, -5.6796, -5.7675, -5.8231, -5.8311, -5.8538, -5.8627, -5.9074, -5.9274, -4.182, -2.1838, -4.7542, -4.979, -5.3156, -5.0679, -3.803, -4.8341, -5.2674, -4.3997, -4.7232, -4.9242, -5.0397, -5.1316, -5.3037, -3.5376, -3.6146, -4.0531, -4.1789, -4.3298, -4.4934, -4.5788, -4.5792, -4.7841, -4.9102, -5.1769, -5.1872, -5.1868, -5.2036, -5.2188, -5.2533, -5.389, -5.4137, -5.4415, -5.5372, -5.5521, -5.5834, -5.5934, -5.6363, -5.76, -5.7813, -5.8206, -5.896, -5.9042, -5.907, -4.5225, -4.4013, -4.8558, -3.7963, -4.9379, -4.8298, -4.9631, -3.6916, -5.3528, -5.1262, -5.387, -5.4059, -3.8811, -4.6683, -4.6744, -4.6804, -4.7, -4.7622, -4.8378, -5.0623, -5.076, -5.1704, -5.1875, -5.191, -5.3077, -5.3175, -5.3624, -5.3806, -5.404, -5.4706, -5.4721, -5.5008, -5.5536, -5.5604, -5.5694, -5.5828, -5.6032, -5.616, -5.6505, -5.6557, -5.6925, -5.7421, -5.1255, -5.017, -5.3042, -5.0909, -3.9608, -3.9817, -3.5182, -5.4022, -5.0435, -5.3649, -4.9747, -5.4672, -3.6842, -4.3499, -4.4944, -4.513, -4.7389, -4.982, -5.0078, -5.0999, -5.1899, -5.2121, -5.2445, -5.2664, -5.4272, -5.478, -5.5911, -5.5912, -5.5963, -5.5999, -5.6117, -5.6163, -5.6634, -5.675, -5.7011, -5.7058, -5.7256, -5.7493, -5.7952, -5.8037, -5.8623, -5.8695, -4.5208, -4.8327, -3.9303, -3.2923, -5.1984, -5.4338, -3.1165, -5.3837, -5.3145, -4.9591, -5.2271, -5.2528, -5.314, -5.5113, -3.3326, -4.4313, -4.5222, -4.5387, -4.6038, -4.6352, -4.6427, -4.7103, -4.8265, -4.9973, -5.0108, -5.071, -5.1353, -5.1634, -5.3269, -5.3657, -5.3859, -5.4625, -5.5355, -5.5586, -5.6047, -5.6174, -5.6419, -5.6564, -5.7254, -5.7362, -5.7806, -5.7809, -5.8448, -5.8695, -2.7109, -3.7931, -3.0261, -4.7333, -4.68, -5.3692, -3.33, -5.0679, -4.5783, -5.427, -5.3415, -5.0941, -5.2198, -5.3466, -5.3596, -3.7874, -4.0694, -4.1607, -4.2469, -4.3618, -4.4512, -4.6364, -4.6477, -4.7419, -4.8915, -4.8904, -4.9059, -4.9074, -4.9739, -5.0489, -5.2059, -5.2584, -5.2752, -5.3505, -5.4072, -5.4142, -5.4202, -5.4735, -5.5321, -5.5506, -5.5522, -5.5819, -5.5884, -5.5976, -5.6071, -3.9543, -4.4183, -4.795, -4.4076, -3.8748, -3.6761, -3.2151, -4.3349, -4.8117, -3.5453, -4.2145, -4.3578, -4.4403, -4.7168, -4.8792, -4.9462, -5.1612, -5.2566, -5.3431, -5.5311, -5.5343, -5.5573, -5.6502, -5.6641, -5.7865, -5.8402, -5.9558, -5.9753, -6.0274, -6.028, -6.03, -6.0496, -6.0534, -6.0945, -6.1335, -6.1348, -6.137, -6.1422, -6.1501, -2.647, -3.1716, -3.9323, -4.6229, -4.913, -2.9595, -5.565, -5.484, -5.5038, -5.4526, -5.7776, -5.5839, -4.5536, -5.4246, -5.643, -5.713], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.6748, 1.674, 1.6739, 1.6738, 1.6735, 1.6733, 1.6732, 1.6731, 1.673, 1.6726, 1.6724, 1.672, 1.6717, 1.6717, 1.6717, 1.6716, 1.6711, 1.6707, 1.6705, 1.6704, 1.6703, 1.6701, 1.6698, 1.6693, 1.669, 1.6689, 1.6688, 1.6687, 1.6684, 1.6683, 1.3766, 0.8672, 1.4208, 1.4588, 1.4873, 1.3008, 0.3108, 0.9315, 1.2982, 0.3251, 0.6412, 0.461, 0.3959, 0.2463, 1.0685, 1.8522, 1.8522, 1.8517, 1.8515, 1.8513, 1.851, 1.8508, 1.8508, 1.8504, 1.85, 1.8491, 1.8491, 1.8491, 1.849, 1.8489, 1.8488, 1.8482, 1.8481, 1.8479, 1.8474, 1.8473, 1.8472, 1.8471, 1.8469, 1.8461, 1.8459, 1.8456, 1.8451, 1.845, 1.845, 1.595, 1.5324, 1.624, 1.3396, 1.6171, 1.5856, 1.6177, 0.4741, 1.6251, 0.9416, 1.4943, 1.2421, 1.947, 1.9456, 1.9456, 1.9456, 1.9455, 1.9454, 1.9451, 1.9444, 1.9443, 1.9439, 1.9439, 1.9438, 1.9433, 1.9432, 1.943, 1.9429, 1.9428, 1.9424, 1.9424, 1.9422, 1.9419, 1.9419, 1.9418, 1.9417, 1.9416, 1.9415, 1.9413, 1.9412, 1.941, 1.9406, 1.809, 1.5264, 1.6819, 1.5055, 0.2049, 0.1321, -0.4672, 1.6368, 0.7711, 1.4056, 0.1613, 1.6435, 1.9714, 1.9704, 1.9701, 1.9701, 1.9695, 1.9687, 1.9686, 1.9682, 1.9678, 1.9677, 1.9675, 1.9674, 1.9666, 1.9663, 1.9655, 1.9655, 1.9655, 1.9655, 1.9654, 1.9654, 1.965, 1.965, 1.9648, 1.9647, 1.9646, 1.9644, 1.964, 1.9639, 1.9634, 1.9634, 1.6656, 1.6964, 1.4548, 0.8215, 1.6778, 1.7749, -0.0655, 1.6824, 1.2167, 0.4188, 0.8757, 0.5332, 0.4515, 0.8366, 2.0109, 2.0093, 2.0091, 2.0091, 2.0089, 2.0088, 2.0088, 2.0086, 2.0082, 2.0076, 2.0075, 2.0073, 2.007, 2.0068, 2.006, 2.0058, 2.0057, 2.0052, 2.0047, 2.0045, 2.0042, 2.0041, 2.0039, 2.0038, 2.0033, 2.0032, 2.0028, 2.0028, 2.0022, 2.002, 1.4548, 1.5713, 1.1808, 1.6424, 1.4359, 1.7099, -0.2789, 1.28, 0.5577, 1.7004, 1.3892, 0.3415, 0.7139, 1.2746, 1.2243, 2.1023, 2.1019, 2.1017, 2.1016, 2.1013, 2.1011, 2.1006, 2.1005, 2.1002, 2.0997, 2.0997, 2.0996, 2.0996, 2.0994, 2.099, 2.0982, 2.0979, 2.0978, 2.0974, 2.097, 2.097, 2.097, 2.0966, 2.0962, 2.096, 2.096, 2.0958, 2.0957, 2.0957, 2.0956, 1.8317, 1.8131, 1.8361, 1.6952, 1.503, 0.4378, -0.1641, 0.3899, 0.7469, 2.1314, 2.1303, 2.13, 2.1298, 2.129, 2.1284, 2.1281, 2.1271, 2.1265, 2.126, 2.1246, 2.1246, 2.1244, 2.1237, 2.1235, 2.1224, 2.1218, 2.1205, 2.1203, 2.1197, 2.1197, 2.1196, 2.1194, 2.1193, 2.1188, 2.1182, 2.1182, 2.1182, 2.1181, 2.118, 1.5599, 1.5532, 1.5032, 1.4896, 1.5166, 0.0915, 1.7714, 1.6997, 1.6677, 1.5997, 1.8715, 1.6544, -0.4397, 0.9511, 0.7291, 1.0429]}, \"token.table\": {\"Topic\": [5, 5, 7, 1, 3, 4, 3, 2, 3, 3, 6, 2, 5, 7, 6, 7, 1, 4, 6, 2, 1, 2, 7, 2, 4, 1, 3, 2, 5, 4, 2, 5, 5, 1, 7, 1, 5, 7, 2, 7, 4, 5, 1, 5, 7, 5, 5, 1, 2, 3, 2, 6, 6, 1, 2, 3, 5, 7, 3, 2, 4, 2, 7, 2, 1, 5, 7, 7, 6, 7, 3, 4, 1, 3, 2, 6, 1, 2, 4, 7, 3, 2, 3, 5, 1, 6, 7, 1, 4, 7, 2, 6, 3, 5, 4, 2, 5, 4, 3, 1, 2, 1, 4, 6, 1, 6, 7, 4, 6, 3, 7, 4, 2, 4, 5, 1, 2, 3, 7, 1, 6, 6, 1, 4, 6, 6, 4, 2, 7, 3, 2, 3, 5, 1, 6, 7, 4, 7, 2, 1, 1, 3, 4, 3, 5, 4, 4, 5, 2, 5, 2, 3, 1, 7, 1, 3, 2, 3, 7, 2, 6, 6, 1, 4, 5, 7, 5, 7, 1, 5, 6, 1, 2, 3, 4, 1, 4, 6, 7, 3, 6, 1, 4, 3, 4, 5, 3, 4, 1, 7, 4, 7, 5, 2, 5, 7, 2, 6, 2, 3, 3, 4, 5, 1, 7, 3, 6, 3, 7, 4, 6, 6, 4, 1, 1, 3, 4, 6, 7, 1, 3, 4, 6, 3, 5, 2, 5, 7, 5, 3, 6, 1, 2, 7, 5, 2, 7, 4, 2, 4, 6, 1, 2, 5, 3, 1, 5, 4, 3, 4, 3, 2, 6, 1, 7, 5, 2, 2, 4, 4, 7, 4, 5, 4, 1, 3, 5, 1, 6, 4, 6, 1, 2, 6, 4, 7, 2, 6, 6, 1, 5, 1, 6, 7, 2, 7, 1, 3, 4, 5, 6, 7, 1, 4, 3, 4, 3, 5, 5, 4, 5, 7, 6, 1, 3, 5, 3, 5, 3, 5, 1, 2, 3, 5, 7, 6, 2, 3, 5, 6, 4, 1, 5, 3, 1, 1, 3, 4, 1, 5, 2, 2, 2, 2, 7, 4, 3, 2, 7, 1, 7, 7, 1, 3, 7, 1, 3, 4, 5, 5, 5, 4, 5, 1, 2, 1, 6, 7, 7, 7, 6, 4, 7, 1, 2, 3, 5, 7, 2, 5, 3, 1], \"Freq\": [0.9976893290621998, 0.6913509005070732, 0.30726706689203254, 0.29708911955382444, 0.10624379109871868, 0.5961457167205881, 0.9933674773415463, 0.9961152631510105, 0.996705753080264, 0.9891436159907643, 0.996002550299745, 0.9967996687649633, 0.9934086710256399, 0.9866472959369427, 0.9945726881722012, 0.9891972986129521, 0.23926145850922295, 0.2119172918224546, 0.5488364884987074, 0.9954588284408729, 0.9947154978103528, 0.9965473733074821, 0.9962693776433785, 0.993739108323186, 0.9923177995017558, 0.9939080408945654, 0.9970514140693633, 0.9910949004471412, 0.9946765531718907, 0.9894181137091776, 0.7662206111617628, 0.2315198968977988, 0.997310834472995, 0.41168165008568713, 0.5888610944263626, 0.3019343795307461, 0.15655856716409056, 0.5395679189762407, 0.22715988219507377, 0.766664602408374, 0.9974332342346893, 0.99683792719877, 0.2783056253806156, 0.18829599932814886, 0.5328156024944871, 0.995293425262798, 0.9949484842731499, 0.20917398311911617, 0.7891563908584838, 0.9979935496032096, 0.999211404571526, 0.9940469315662208, 0.9958026811332826, 0.22217357234434384, 0.17380925727618735, 0.30832250855949755, 0.1662523330467879, 0.12846771189979064, 0.9988552645323868, 0.7965918416786812, 0.20282227703257938, 0.9986617652724987, 0.9939668725425836, 0.998676572514781, 0.9978732380029948, 0.9958403450229304, 0.9938312301275266, 0.9961282654095261, 0.9957759322061646, 0.994117471330476, 0.9920399323421818, 0.9974000253774102, 0.9990965138758134, 0.9968352329180272, 0.9959411189310619, 0.9977100050932798, 0.4556434073825747, 0.5425982561197072, 0.9920075831054388, 0.9993292074693807, 0.9969519941882398, 0.2104097387201468, 0.22470942970112764, 0.5617735742528192, 0.9967898415940024, 0.9906169618520041, 0.986472157961678, 0.9983375054908303, 0.9974730624153445, 0.9810724153796426, 0.9944603737626637, 0.9981708766245642, 0.2568973919975397, 0.7385800019929265, 0.9940800133162747, 0.798117876246714, 0.1983201995522138, 0.9934423748935519, 0.997817412580729, 0.99791466895992, 0.9947871616774674, 0.9983178749609157, 0.2499398342479666, 0.7475264767416249, 0.9953395514057128, 0.34469298764444556, 0.6477850974697339, 0.2364709699024498, 0.7622884060830525, 0.9943567651510538, 0.9965510114799931, 0.989548475070559, 0.2437882126566572, 0.27426173923873937, 0.47741858311928703, 0.5437625126941315, 0.050152853112565524, 0.15837743088178585, 0.24548501786676807, 0.9965346500490676, 0.9910507686505424, 0.9969915236516795, 0.9937018647909239, 0.9923928592847905, 0.9952945582024304, 0.9959770649764484, 0.9965122734271956, 0.9953289558025571, 0.9835957928708925, 0.9940589386487447, 0.25193757187465976, 0.1749324316822897, 0.5730344772051085, 0.23253887516536903, 0.7660104123094509, 0.9974261598609045, 0.9909363494968977, 0.9986435869436291, 0.9990701767351952, 0.9950361460431103, 0.6865083992640139, 0.14729682129802596, 0.1657089239602792, 0.996001549932578, 0.9977390848908655, 0.9947632567101616, 0.9979535441345662, 0.9976332445926215, 0.7252763836798433, 0.2724042755605045, 0.997182229691743, 0.9971753946992138, 0.9956188877551498, 0.9816834562592688, 0.99709528615095, 0.9950835821407454, 0.7902601889190555, 0.2081343707441134, 0.994028391785804, 0.9961234912340878, 0.9975680466587997, 0.989969310620295, 0.32544009476118146, 0.1317257526414306, 0.20533720264693592, 0.33706295528836655, 0.7302835523354542, 0.26402559199820264, 0.9959087142220268, 0.9972510260933135, 0.9987900296412116, 0.47488632128771874, 0.1367096985525251, 0.168368786638373, 0.21873551768404015, 0.9966957598219784, 0.33469124324818117, 0.6653500618789143, 0.9910454711984567, 0.9957259001286902, 0.9967089229029608, 0.6853707335303204, 0.31065869697402376, 0.19579739806139657, 0.3220351941799286, 0.48176465049317313, 0.24832726468497612, 0.7502653528780129, 0.3765836310986055, 0.621362991312699, 0.7596313332133545, 0.2377707831602776, 0.9966228331212427, 0.9947984624617882, 0.4357342906210367, 0.5641229905677494, 0.9907865701162578, 0.9894289495075704, 0.2622413282532038, 0.7301621296461752, 0.9931407101762637, 0.995932462579772, 0.9920552633832381, 0.9980370457274816, 0.9960289152941085, 0.9957006896184288, 0.9987227024021148, 0.9885554904423075, 0.9991562852536384, 0.9952277329402617, 0.9973649702346284, 0.9907875702086995, 0.9990737261257954, 0.9949898198900368, 0.2554834509539248, 0.16278103246524367, 0.3164573631146347, 0.18899153769269814, 0.07642431524215677, 0.9995189830585541, 0.6547566922692903, 0.1472419355820892, 0.194234042682756, 0.9948823736488238, 0.9965713360515639, 0.21172229454570654, 0.2605812855947157, 0.5252341537768489, 0.9909692710305932, 0.7656623505171593, 0.22921102212934066, 0.9959174720497576, 0.995575813626527, 0.9842514021891573, 0.9929707728780631, 0.6982797908608607, 0.2986353822549593, 0.9912874225615417, 0.9968245424623149, 0.7365312273834668, 0.2630468669226667, 0.3229325132767987, 0.22181223134163955, 0.4566722409974932, 0.9935976481646881, 0.9990831692623089, 0.9967678193300945, 0.9951934639595988, 0.9983118861359848, 0.9986816927083414, 0.9933548249032759, 0.9984851693204391, 0.9933149273692142, 0.9923988530456495, 0.9908281325240916, 0.9961426303373014, 0.998488120159245, 0.9948439894660188, 0.9922814665829754, 0.9971466335389622, 0.9889751836237825, 0.9909685276950395, 0.9977938022688582, 0.994915027413886, 0.12971960934892537, 0.5817727934436654, 0.2869554994688349, 0.9980393317400552, 0.9885770381277857, 0.9937980957676014, 0.9975828561094001, 0.9963503473042555, 0.999084147875055, 0.9950687150675087, 0.8226022593056181, 0.17670715199898465, 0.9977418184204223, 0.9929200095790182, 0.9950974777963733, 0.9985051093113461, 0.9905478593420892, 0.259214789430245, 0.18043382401517052, 0.5601072508865294, 0.9941762751623447, 0.9876598153623105, 0.44571200472901523, 0.08931397528466366, 0.13030117845692127, 0.1012288599277618, 0.10351651777923665, 0.12991990214834212, 0.7758792701295104, 0.22322783470206586, 0.9941969444277345, 0.9932087086744049, 0.9981689433265719, 0.99385519692521, 0.9901151783796868, 0.26823622757283716, 0.5364724551456743, 0.19267672684809428, 0.9920504876266635, 0.25964506075448723, 0.7347402783052511, 0.9946995348989228, 0.9976871465937558, 0.9970663265216723, 0.6407562995771968, 0.35670969254813023, 0.2569895046217647, 0.40105937842487516, 0.18495456772020943, 0.1538043247357531, 0.9816749226813813, 0.9917704828381639, 0.7733712472882786, 0.2271010805529072, 0.9992320025572828, 0.9913382427550959, 0.9968097748008716, 0.9992372765907107, 0.9930635603878907, 0.9976752039197496, 0.9928651876802452, 0.9992014320541949, 0.9978800412414679, 0.9944146448187411, 0.3555273140871269, 0.6436104222498665, 0.997429769695851, 0.9917024773101931, 0.9980615437327282, 0.3698599539946578, 0.6281748424988634, 0.9960049290300702, 0.9902908529120162, 0.29769323560069455, 0.6992329487365151, 0.8283771326224965, 0.17054823318698456, 0.9868626605958641, 0.12506710416712447, 0.8708376142007185, 0.986845171328662, 0.15472825139804747, 0.2259032470411493, 0.4703738842500643, 0.14853912134212557, 0.9981175717225024, 0.994421728303534, 0.9958895106360384, 0.9940419964779151, 0.8060817883211859, 0.19165580981063163, 0.741801535327211, 0.257407472826477, 0.9855935354608593, 0.9960621322660487, 0.98358765883985, 0.989146594635761, 0.7428034547409061, 0.25342706102925033, 0.9965584216831578, 0.598841808587056, 0.16715430764657901, 0.23386267812938807, 0.9905875002169345, 0.9964854623459054, 0.9944012275265877, 0.9955865099613987, 0.9936646517272226], \"Term\": [\"2_people\", \"Drop\", \"Drop\", \"EVERYTHING\", \"EVERYTHING\", \"EVERYTHING\", \"Farm\", \"Followed\", \"Had\", \"Management\", \"Oprah\", \"Sitting\", \"Something\", \"Things\", \"Yum\", \"ages\", \"aioli\", \"aioli\", \"aioli\", \"app\", \"appetizer\", \"artifacts\", \"aspect\", \"bacon\", \"basketball_game\", \"be_going_back\", \"been_driving\", \"beer_before\", \"being\", \"better\", \"beverage\", \"beverage\", \"bill\", \"bites\", \"bites\", \"blah_blah_blah\", \"blah_blah_blah\", \"blah_blah_blah\", \"blue_cheese\", \"blue_cheese\", \"bombs\", \"boss\", \"bread\", \"bread\", \"bread\", \"breakfast/brunch_place\", \"brought\", \"bucks_for_dinner\", \"bucks_for_dinner\", \"building\", \"bunch_of\", \"burgers\", \"but_none\", \"butter\", \"butter\", \"butter\", \"butter\", \"butter\", \"by_mistake\", \"by_myself\", \"by_myself\", \"called\", \"can_choose\", \"candy\", \"carafe\", \"carbonara\", \"cards_are\", \"charging\", \"cheese\", \"choice\", \"choice_for\", \"choices\", \"chopped\", \"cinammoness\", \"claiming\", \"coconut_milk\", \"cold\", \"cold\", \"come_here\", \"comeback\", \"comfort\", \"construction\", \"construction\", \"construction\", \"costs\", \"course\", \"court\", \"crisp\", \"curry\", \"deli\", \"disappoint\", \"disappointment\", \"dish\", \"dish\", \"dishes\", \"door\", \"door\", \"drink\", \"drums\", \"dumplings\", \"eating\", \"employees\", \"entrees_are\", \"entrees_are\", \"entrees_before_we\", \"even_if\", \"even_if\", \"even_know_how_much\", \"even_know_how_much\", \"events/pool\", \"ever_eaten\", \"every_time\", \"everything\", \"everything\", \"everything\", \"everything_on\", \"everything_on\", \"everything_on\", \"everything_on\", \"favor\", \"favorites\", \"fiance\", \"fills_up\", \"finished\", \"finished_off\", \"fixin\", \"flavors\", \"food\", \"freebies\", \"fried_steak\", \"garden\", \"garden\", \"garden\", \"gave\", \"gave\", \"get_some\", \"get_your\", \"girl\", \"girls\", \"glass_of\", \"go_back\", \"go_back\", \"go_back\", \"go_wrong\", \"good_lighting\", \"goods\", \"great_experience\", \"great_place\", \"griping\", \"griping\", \"grounds\", \"group\", \"guy\", \"had_never_been_there\", \"hamburger_bun\", \"have_been_coming_here_for\", \"have_been_here\", \"have_been_here\", \"have_gone_after\", \"have_never_made\", \"helped\", \"her_fiance\", \"hey\", \"hey\", \"hey\", \"hey\", \"horchata\", \"horchata\", \"host\", \"hot_sauce\", \"how_long\", \"idea\", \"idea\", \"idea\", \"idea\", \"including\", \"issues\", \"issues\", \"jar_of\", \"joy\", \"kick\", \"knew\", \"knew\", \"leaves\", \"leaves\", \"leaves\", \"left\", \"left\", \"left_super\", \"left_super\", \"let_me\", \"let_me\", \"list\", \"live_music\", \"looked_like\", \"looked_like\", \"love_having\", \"love_love\", \"love_their\", \"love_their\", \"maitre\", \"mall\", \"marg\", \"mass\", \"melts\", \"menu\", \"mmmm\", \"mom\", \"monotone\", \"moseyed\", \"mozzarella\", \"mushrooms\", \"one\", \"options\", \"order\", \"order\", \"order\", \"order\", \"order\", \"ordered\", \"orders\", \"orders\", \"orders\", \"others\", \"overlooking\", \"pancakes\", \"pancakes\", \"pancakes\", \"par\", \"part_of\", \"part_of\", \"patio\", \"peeves\", \"people\", \"perfect_for_me\", \"person\", \"person\", \"pesto\", \"pho\", \"pieces_of\", \"pieces_of\", \"pizza\", \"pizza\", \"pizza\", \"place\", \"placed_our\", \"places_near_the\", \"plan_on\", \"please_everyone\", \"pleasure\", \"point\", \"powdery\", \"pretty_large\", \"prices\", \"problem\", \"produced\", \"providing\", \"race\", \"radishes\", \"rate\", \"recalling\", \"received\", \"recommended_over\", \"refill_our\", \"restaurants\", \"restaurants\", \"restaurants\", \"reviewers_have_some\", \"reviews\", \"rice\", \"rings\", \"romaine_lettuce\", \"salsa\", \"sandwich\", \"sausage\", \"sausage\", \"saying\", \"scrambled\", \"seasoned\", \"seat\", \"selling\", \"server\", \"server\", \"server\", \"servers\", \"sides\", \"sitting\", \"sitting\", \"sitting\", \"sitting\", \"sitting\", \"sitting\", \"situation\", \"situation\", \"six-pack\", \"size\", \"soggy\", \"something\", \"sounding\", \"spaghetti\", \"spaghetti\", \"spaghetti\", \"splitting\", \"spot\", \"spot\", \"standing\", \"steep\", \"summer\", \"sun\", \"sun\", \"tacos\", \"tacos\", \"tacos\", \"tacos\", \"talents\", \"tap\", \"tart\", \"tart\", \"tasting\", \"tazer\", \"things\", \"thought\", \"thoughts\", \"thumbs\", \"thyme\", \"tip\", \"tiramisu\", \"tofu\", \"tomato\", \"tomato\", \"topping\", \"two-thirds\", \"uprooted\", \"value\", \"value\", \"veggies_were\", \"waited\", \"waiter_came\", \"waiter_came\", \"wanted\", \"wanted\", \"was_hoping_for\", \"was_outstanding\", \"was_outstanding\", \"was_still\", \"way\", \"way\", \"way\", \"way\", \"weekday\", \"weeks\", \"went_outside\", \"went_with\", \"were\", \"were\", \"were_awesome\", \"were_awesome\", \"were_planning_on\", \"whether_or\", \"whisked\", \"white_truffle\", \"wind_up\", \"wind_up\", \"winner\", \"wooden\", \"wooden\", \"wooden\", \"word\", \"working\", \"would_be\", \"yelp\", \"yummy\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [7, 1, 2, 5, 3, 6, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el24780132965858884855847118\", ldavis_el24780132965858884855847118_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el24780132965858884855847118\", ldavis_el24780132965858884855847118_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el24780132965858884855847118\", ldavis_el24780132965858884855847118_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "6      0.134885  0.056317       1        1  18.724604\n",
       "0     -0.291262 -0.138211       2        1  15.677214\n",
       "1     -0.087765 -0.113505       3        1  14.252538\n",
       "4      0.156386 -0.134779       4        1  13.912512\n",
       "2     -0.160825  0.188946       5        1  13.377197\n",
       "5      0.191576 -0.107004       6        1  12.201221\n",
       "3      0.057004  0.248236       7        1  11.854715, topic_info=               Term          Freq         Total Category  logprob  loglift\n",
       "12      looked_like   3302.000000   3302.000000  Default  30.0000  30.0000\n",
       "17           garden   3441.000000   3441.000000  Default  29.0000  29.0000\n",
       "114         ordered   1539.000000   1539.000000  Default  28.0000  28.0000\n",
       "3           sitting  10491.000000  10491.000000  Default  27.0000  27.0000\n",
       "61           server   1967.000000   1967.000000  Default  26.0000  26.0000\n",
       "...             ...           ...           ...      ...      ...      ...\n",
       "1185     left_super     98.784882    159.327159   Topic7  -5.5839   1.6544\n",
       "19            order    276.803981   3624.500908   Topic7  -4.5536  -0.4397\n",
       "184            Drop    115.851901    377.521747   Topic7  -5.4246   0.9511\n",
       "263   everything_on     93.113962    378.841857   Topic7  -5.6430   0.7291\n",
       "617             hey     86.819771    258.112019   Topic7  -5.7130   1.0429\n",
       "\n",
       "[333 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "438       5  0.997689    2_people\n",
       "184       5  0.691351        Drop\n",
       "184       7  0.307267        Drop\n",
       "23        1  0.297089  EVERYTHING\n",
       "23        3  0.106244  EVERYTHING\n",
       "...     ...       ...         ...\n",
       "2393      7  0.990588        word\n",
       "663       2  0.996485     working\n",
       "499       5  0.994401    would_be\n",
       "704       3  0.995587        yelp\n",
       "401       1  0.993665       yummy\n",
       "\n",
       "[372 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[7, 1, 2, 5, 3, 6, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "texts = data_prueba_csv\n",
    "texts = [text for text in texts if len(text) > 0]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(topic_model=ldamodel_new, corpus=corpus, dictionary=dictionary)\n",
    "#TypeError: drop() takes from 1 to 2 positional arguments but 3 were given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bacd7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82bacd7c",
    "outputId": "0db96537-9dee-449a-ad6a-b173e37ff48a"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "print(\"La incorporación de verbos mejora o no los resultados?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad2956-bf8d-4869-8954-c0f46b354444",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">Bajo mi punto de vista, la incorporación de verbos no mejora los resultados, si no que introduce más ruido.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ed8c2",
   "metadata": {
    "id": "1f6ed8c2"
   },
   "source": [
    "# 3. Clasificación (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32290b6e",
   "metadata": {
    "id": "32290b6e"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Crea un clasificador automático de opiniones positivas y negativas. (0.75 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45ae89",
   "metadata": {
    "id": "3a45ae89"
   },
   "source": [
    "<i>Primer paso</i>: realizamos dos listas. Una con los textos y otra con las etiquetas de valoración (0 y 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec874492",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec874492",
    "outputId": "4bc940e8-e684-487d-e27a-7d328955ae95",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "opinions = df['text'].to_list()\n",
    "labels = df['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720bc32",
   "metadata": {
    "id": "9720bc32"
   },
   "source": [
    "<i>Segundo paso</i>: Vectorizamos las opiniones con un vectorizador tf.idf. Usad 'word' como analyzer (0.25 punts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fbcfdd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fbcfdd4",
    "outputId": "255e55ec-a18c-494d-e593-7e3bb3429f82",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                 #\n",
    "#############################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tif_vectorizer = TfidfVectorizer(\n",
    "    analyzer= 'word')\n",
    "tif_vectorizer_fit = tif_vectorizer.fit_transform(opinions)\n",
    "X = tif_vectorizer_fit.toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3122e026",
   "metadata": {
    "id": "3122e026"
   },
   "source": [
    "<i>Tercer paso</i>: Preparamos el corpus de entrenamiento y evaluación (0.25 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b251acc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b251acc",
    "outputId": "f99f0b5a-b4f0-4d29-d5e6-b18542f8f011",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, labels, train_size=0.80, random_state=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaecbf4",
   "metadata": {
    "id": "0aaecbf4"
   },
   "source": [
    "<i>Cuarto paso</i>: Entrenar al clasificador con Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39791e56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39791e56",
    "outputId": "d149d93c-6023-4eb1-e076-75788b888d34",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "log_model = classifier.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78a34d",
   "metadata": {
    "id": "8c78a34d"
   },
   "source": [
    "Quinto paso<i>: Utilizar el modelo entrenado para predecir la categoría 1 (positivo) o 0 (negativo) de las opiniones del conjunto de test y mostrar las palabras más informativas para cada categoría. (0.25 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "559b292b-e200-4add-8401-60f68e6d90ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negativo', 'Positivo'], dtype='<U8')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(classifier.classes_==1, 'Positivo', 'Negativo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "160fc014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "160fc014",
    "outputId": "41a3aea5-f3d5-4872-a3a1-2c2e78e8f696",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLASES POSITIVAS/NEGATIVAS DEL CORPUS DE TEST:\n",
      "===============\n",
      "[1 1 1 ... 1 1 1]\n",
      "\n",
      "FEATURES MÁS INFORMATIVOS EN LAS OPINIONES POSITIVA Y NEGATIVAS:\n",
      "===============\n",
      "Negativo -4.376775576963335 not\n",
      "Negativo -3.4084638498683764 no\n",
      "Negativo -2.8463753188228496 ok\n",
      "Negativo -2.4902612215150466 bland\n",
      "Negativo -2.360421604773643 horrible\n",
      "Negativo -2.347624536673859 bad\n",
      "Negativo -2.3213252892895766 was\n",
      "Negativo -2.3093270259990972 wasn\n",
      "Negativo -2.288174295951619 mediocre\n",
      "Negativo -2.2322553757462384 better\n",
      "Positivo 5.33728051728932 great\n",
      "Positivo 3.3127160358936543 delicious\n",
      "Positivo 3.142836715113883 good\n",
      "Positivo 3.040001351761747 love\n",
      "Positivo 2.7845127848352056 best\n",
      "Positivo 2.7184735996650833 amazing\n",
      "Positivo 2.6688238076751256 and\n",
      "Positivo 2.6687014875332205 excellent\n",
      "Positivo 2.593323809998095 awesome\n",
      "Positivo 2.553833822136378 definitely\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "y_pred = log_model.predict(X_test)\n",
    "\n",
    "print(\"\\nCLASES POSITIVAS/NEGATIVAS DEL CORPUS DE TEST:\\n===============\")\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=10):\n",
    "    class_labels = np.where(classifier.classes_==1, 'Positivo', 'Negativo')\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "\n",
    "    for coef, feat in topn_class1:\n",
    "        print (class_labels[0], coef, feat)\n",
    "\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print (class_labels[1], coef, feat)\n",
    "\n",
    "print(\"\\nFEATURES MÁS INFORMATIVOS EN LAS OPINIONES POSITIVA Y NEGATIVAS:\\n===============\")\n",
    "\n",
    "print(most_informative_feature_for_binary_classification(tif_vectorizer, classifier))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af23001",
   "metadata": {
    "id": "4af23001"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercicio:</strong> Muestra sobre qué aspectos se hacen valoraciones negativas. (0.75 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb0943",
   "metadata": {
    "id": "d8eb0943"
   },
   "source": [
    "<i>Primer paso<i>: Elige dos palabras más informativas de la categoría 0 y toma un conjunto de opiniones en las que aparezcan estas palabras. Preprocesa las opiniones quitando los caracteres de salto de línea (0.25 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "785f7d61-237e-4fc0-937b-b8781aaaa05a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I've eaten here many times, but none as bad as last night. Service was excellent, and highly attentive. Food, absolutely horrible. My expectation was they would serve a steak on par with their seafood. After all, they were charging 39 bucks for a ribeye. What I was hoping for was a 1- 1-1/2' thick steak, cooked Pittsburgh style as I had ordered. What I got a a 3/4 in thick piece of meat that was mostly fat, gristle, and in no way resembled Pittsburgh Style. Salad, similar to something you could get at Chick Filet Veggies, blah. Bread basket, ample, but day old, and if not, it certainly wasn't fresh. In addition to bad food, we were crammed into a small room where we were nuts to butts with 6 other tables, listening to conversations ranging from someone's recent bout with pinkeye, and another couple who elected to speak entirely in French, until the waiter showed up, then it was like they turned off the French switch and suddenly began speaking English. I've had it with this place. If I'm going to pay 150 bucks for dinner, it'll be at Mortons, or Maestro where the steaks are 1-1/2 in thick, cooked to perfection, and half of it doesnt wind up on the plate as fat and gristle\",\n",
       " \"Sacks is a different kind of sandwich shop. They have some crazy ingredients like varied sauces and mayos, fresh spinach, grilled peppers, and all sorts of other things you don't see at a typical deli. Then they serve it up on a baguette, wrap, or wheat bread for about $6. The specials are particularly diverse and change daily. On the other side of the coin, their website is horrible and occasionally we find some things missing or not quite right, but we chalk that up to charm and whimsy. (We get a weekly Sacks delivery.) If you want a basic ham and cheese sandwich, you can get that here. If you want a sandwich like their Blue Bayou, which is spicy marinated chicken served with bacon, bleu cheese crumbles, cheddar jack, mushrooms, spinach, and chipotle mayo, all melted down, well, you can get that here too, but only on the days when it's a special. From their regular, artist-inspired menu, their Dali is an instant classic take on the egg salad: This one comes with lettuce, bacon, cream cheese, mayo, and tomato, on 12-grain bread. The Encore has prime steak, melted provolone, hot/mild/or mixed peppers, feta yogurt or marinara sauce, lettuce, tomato, and parmesan for $6.30. Seriously, give this place a shot and you won't be disappointed. Oh, and every sandwich comes with a lovely cookie. Brilliant.\",\n",
       " \"- the location is excellent - the food is mediocre, and milder than they advertise - the wait staff is polite and bland. At several opportunities, they were missing for more than 5 mins. - the bill will be higher than anyone expects Bottom line: don't bother\",\n",
       " \"I read all the good reviews of this restaurant , and wanted to try it, as it's right up the street from where I live. I don't think I hit a bad night, it was slow, the service was horrible, they confused our order with the table next to us. He put plates of food down and walked off, the order had nothing to do with ours. We tried to get his attention,gave up and took a bite. OMG, it was AWFUL,fried fish that tasted so fishy that I couldn't eat it. At that point, the table next to us realized it was their meal , and got up and walked out . Finally I got the waiters attention, he took the plates ,gave us the correct meal, and didn't offer to do anything about all of our time he had wasted, let alone the mistake. Our real meal was O.K., I tried the tamales, wasn't blown away,my boyfriend said his steak was so-so. I don't think I would come back again.\",\n",
       " 'Decentfood, but horrible service. Both servers walked by me three times while I was sitting inside without acknowledging me at all. I was by myself and felt as though everyone else that came walking in were immediately welcomed. I wish I would have received the same attention. I was ready to walk out when i was finally approached. Nobody came back to check on me to make sure everything was ok after they delivered my food. Being from Vegas I am not accustomed to such non attentive service staff. I guess that flys here in Scottsdale, but I expected more from the type of clientele that they have. On a positive note, they do have a cozy place that gives the feel of a French coffee shop/bistro. Hopefully this review will be helpfull in giving them insight into making sure that every guest is treated with the same attentiveness to service.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions_preprocessed = [opinion.replace(r\"(\\\\|\\n|\\\\')\", ' ') for opinion in opinions]\n",
    "opinions_preprocessed[0:5]\n",
    "worst_words = [\"horrible\", \"mediocre\"]\n",
    "\n",
    "opinions_filtered = [opinion for opinion in opinions_preprocessed if any(word in opinion for word in worst_words)]\n",
    "opinions_filtered[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c24dbb",
   "metadata": {
    "id": "b5c24dbb"
   },
   "source": [
    "<i>Segundo paso<i>: Utiliza el diccionario de opiniones (archivo AFINN-111) para extraer la polaridad de cada opinión como la media de los valores de las opinion words del texto. (0.25 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "895f6b09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "895f6b09",
    "outputId": "ff7b7873-2168-491e-d699-4405de10895f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abandon', '-2'],\n",
       " ['abandoned', '-2'],\n",
       " ['abandons', '-2'],\n",
       " ['abducted', '-2'],\n",
       " ['abduction', '-2'],\n",
       " ['abductions', '-2'],\n",
       " ['abhor', '-3'],\n",
       " ['abhorred', '-3'],\n",
       " ['abhorrent', '-3'],\n",
       " ['abhors', '-3']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "#Creamos una lista de opinion words y pesos a partir del diccionario AFINN\n",
    "opinion_words_file = 'AFINN-111.txt'\n",
    "\n",
    "opinion_words = []\n",
    "\n",
    "with open(opinion_words_file, 'r') as of:\n",
    "    ol = of.readlines()\n",
    "    for wo in ol:\n",
    "        w, o = wo.strip().split('\\t')\n",
    "        opinion_words.append([w,o])\n",
    "\n",
    "opinion_words_df = pd.DataFrame(opinion_words, columns =['opinion_word', 'polarity'])\n",
    "head(opinion_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11e8d859-e2ec-4c50-833d-2c2760bf2d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_np(candidate):\n",
    "    test = False\n",
    "    tokens = candidate.split()\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    if len(tagged_tokens) > 1:\n",
    "        PoS_initial = tagged_tokens[0][1][:2]\n",
    "        PoS_final = tagged_tokens[-1][1][:2]\n",
    "        if ((PoS_initial == 'NN' or PoS_initial == 'JJ') and (PoS_final == 'NN' or PoS_final == 'NS')):\n",
    "            test = True\n",
    "    else:\n",
    "        if len(candidate) > 1 and tagged_tokens[0][1][:2] == 'NN' or tagged_tokens[0][1][:2] == 'NS':\n",
    "            test = True \n",
    "    return test\n",
    "\n",
    "def get_np_candidates(text):\n",
    "    #Definimos las métricas que evaluaran si un n-grama puede ser una collocation\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    #Tokenizamos y obtenemos los tokens del texto\n",
    "    tokens = [w for w in word_tokenize(text.lower())]\n",
    "    #Búsqueda de bigramas\n",
    "    bigramfinder = BigramCollocationFinder.from_words(tokens)\n",
    "    #Se toman los bigramas que no tienen signos de puntuación, etc.\n",
    "    bigramfinder.apply_word_filter(lambda w: (re.match(r'\\W', w)))\n",
    "    #N mejores candidatos a ser colocaciones, tras pasar por el cálculo del PMI\n",
    "    bigram_candidates = bigramfinder.nbest(bigram_measures.pmi,100)\n",
    "    #Transformación de la tupla del bigrama a collocation\n",
    "    collocation_candidates = [\" \".join(bc) for bc in bigram_candidates]\n",
    "    #Elegimos los candidatos que son sintagmas nominales\n",
    "    np_candidates = [c for c in tokens + collocation_candidates if is_np(c) == True]\n",
    "    return np_candidates\n",
    "\n",
    "opinion = \"I read all the good reviews of this restaurant , and wanted to try it, as it's right up the street from where I live. I don't think I hit a bad night, it was slow, the service was horrible, they confused our order with the table next to us. He put plates of food down and walked off, the order had nothing to do with ours. We tried to get his attention,gave up and took a bite. OMG, it was AWFUL,fried fish that tasted so fishy that I couldn't eat it. At that point, the table next to us realized it was their meal , and got up and walked out . Finally I got the waiters attention, he took the plates ,gave us the correct meal, and didn't offer to do anything about all of our time he had wasted, let alone the mistake. Our real meal was O.K., I tried the tamales, wasn't blown away,my boyfriend said his steak was so-so. I don't think I would come back again.\"\n",
    "np_candidates = get_np_candidates(opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "06ec3ab9-dce2-4440-8388-9b073a9e1cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "polarity_list = []\n",
    "\n",
    "for i in range (0, len(opinions_filtered)):\n",
    "    total_polarity = 0\n",
    "    total_values  = 0\n",
    "    opinion_words_list = []\n",
    "    opinion =  opinions_filtered[i]\n",
    "    #token_list =get_np_candidates(opinion.lower())    \n",
    "    #print(\"token_list\", token_list)\n",
    "    for ow in word_tokenize(opinion.lower()):\n",
    "        if opinion_words_df['opinion_word'].isin([ow]).any():\n",
    "            #print(ow)\n",
    "            index = opinion_words_df['opinion_word'][opinion_words_df['opinion_word'] == ow].index[0]\n",
    "            #print(\"index\", int(opinion_words_df['polarity'][index]))\n",
    "            total_values += 1\n",
    "            total_polarity += int(opinion_words_df['polarity'][index])\n",
    "            opinion_words_list.append(ow)\n",
    "    #print(total_values)\n",
    "    #print(total_polarity)\n",
    "    if(total_values > 0):\n",
    "        mean_polarity = total_polarity / total_values\n",
    "    else:\n",
    "        mean_polarity = 0\n",
    "    polarity_list.append({\"polarity\": mean_polarity, \"opinion\": i, \"words\":opinion_words_list })           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561cfe1",
   "metadata": {
    "id": "1561cfe1"
   },
   "source": [
    "Tercer paso: Selecciona opiniones con polaridad negativa que ejemplifiquen los aspectos peor valorados. Comenta cuáles son estos aspectos (0.25 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cdccdc41-94f3-421d-ab86-fd57a1898082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     polarity  opinion                                              words\n",
      "6   -3.000000        6                                         [horrible]\n",
      "107 -3.000000      107                                         [horrible]\n",
      "9   -3.000000        9                               [horrible, horrible]\n",
      "141 -2.333333      141                             [bad, horrible, waste]\n",
      "97  -2.000000       97                                  [horrible, avoid]\n",
      "118 -1.800000      118         [horrible, smile, gross, worst, desperate]\n",
      "3   -1.750000        3  [good, bad, horrible, confused, awful, wasted,...\n",
      "93  -1.714286       93    [hate, horrible, no, no, pay, no, disappointed]\n",
      "108 -1.500000      108            [good, wrong, no, disdain, awful, fuck]\n"
     ]
    }
   ],
   "source": [
    "polarity_list = pd.DataFrame(polarity_list)\n",
    "print(polarity_list.sort_values(by=['polarity'])[1:10])\n",
    "opiniones = polarity_list.sort_values(by=['polarity'])[1:10][\"opinion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3df96966-e286-4600-a4f5-f5d2e7a561e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Service was horrible and food was mediocre at best~ won't be going back\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions_filtered[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2f81ebf8-5a55-461b-a253-ff1c10cc6697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This place is over priced and the service is horrible. I went there with my feiends and the server was very rude and degrading!'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions_filtered[107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f2bf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "412f2bf5",
    "outputId": "24deaf9b-edbc-4d29-e9f5-944a5a4746e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "opinion_words_df['opinion_word'][opinion_words_df['opinion_word'] == \"abhors\"].index[0]\n",
    "opinion_words_df['polarity'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d8d2f",
   "metadata": {
    "id": "b69d8d2f"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Obtén los resultados de las métricas de evaluación del clasificador basado en regresión logística. Obtén también los resultados de las métricas de evaluación cuando el modelo del clasificador es diferente. Por ejemplo, un modelo SVM (0.25 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "afc6ad8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afc6ad8f",
    "outputId": "51da2179-7a5e-4216-e99b-36610b4a5276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.46      0.62       240\n",
      "           1       0.88      0.99      0.93       931\n",
      "\n",
      "    accuracy                           0.88      1171\n",
      "   macro avg       0.91      0.73      0.77      1171\n",
      "weighted avg       0.89      0.88      0.87      1171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN MODELO REGRESIÓN LOGÍSTICA              #\n",
    "#############################################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "#Modelo elegido: logistic regression\n",
    "logreg_model = LogisticRegression()\n",
    "#Entrenamiento\n",
    "logreg_model.fit(X=X_train, y=y_train)\n",
    "#Clasificación\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "#Métricas de evaluación\n",
    "#https://stats.stackexchange.com/questions/117654/what-does-the-numbers-in-the-classification-report-of-sklearn-mean\n",
    "bm = metrics.classification_report(y_test, y_pred, labels=[0,1])\n",
    "\n",
    "print(bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "210a7df8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "210a7df8",
    "outputId": "9c34cd1b-9fb3-4f44-da5c-43f672110ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.70      0.77       240\n",
      "           1       0.93      0.97      0.95       931\n",
      "\n",
      "    accuracy                           0.92      1171\n",
      "   macro avg       0.90      0.84      0.86      1171\n",
      "weighted avg       0.91      0.92      0.91      1171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN MODELO SVM                       #\n",
    "#############################################\n",
    "import sklearn.svm\n",
    "\n",
    "classifier = sklearn.svm.LinearSVC()\n",
    "#Entrenamiento\n",
    "svm_model = classifier.fit(X=X_train, y=y_train)\n",
    "#Clasificación\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "svm = metrics.classification_report(y_test, y_pred, labels=[0,1])\n",
    "\n",
    "print(svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e4564",
   "metadata": {
    "id": "ce2e4564"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Compara los dos modelos en función de estas métricas de evaluación. (0.25 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0466babb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0466babb",
    "outputId": "856d7009-8fca-4fc7-d7f0-00e3649d41d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decir cuál de los dos modelos tiene mejores valores de precision recall y f1.\n",
      "Comentar si las diferencias son significativas y apuntar las causas por las cuales un modelo da mejores resultados que el otro.\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "print(\"Decir cuál de los dos modelos tiene mejores valores de precision recall y f1.\\nComentar si las diferencias son significativas y apuntar las causas por las cuales un modelo da mejores resultados que el otro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df464e-f959-4450-a9b4-55c1cea82e48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe240b14",
    "outputId": "158f66c0-26c7-4501-9bad-dc113e25e83b"
   },
   "source": [
    "<div style=\"color:blue\">Según los resultados que vemos arriba, el modelo que mejores resultados da es SVM </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbd6c5-d685-4de2-850f-ad4d06a6ad2c",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "\n",
    "- Joaquim Moré, Cómo interpretar y analizar automáticamente la información textual\n",
    "- PLA1, Scripts de los métodos explicados en el módulo\n",
    "- Joaquim Moré, Extracción de sentimientos y opiniones\n",
    "- PLA2, Scripts de los métodos explicados en el módulo\n",
    "- Joaquim Moré, Evaluación de la calidad de los sistemas de reconocimiento de sentimientos\n",
    "- PLA3, Scripts de los métodos explicados en el módulo\n",
    "- https://campus.datacamp.com/courses/introduction-to-natural-language-processing-in-python/regular-expressions-word-tokenization?ex=1\n",
    "\n",
    "- https://dplyr.tidyverse.org/reference/case_when.html\n",
    "- https://datatofish.com/numpy-array-to-pandas-dataframe/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
